{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ETL Pipeline –¥–ª—è Yandex Handbook\n",
    "\n",
    "**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**\n",
    "- **Child chunks** (450 —Ç–æ–∫–µ–Ω–æ–≤) ‚Üí Qdrant –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ (HYBRID: dense + sparse)\n",
    "- **Parent chunks** (1500 —Ç–æ–∫–µ–Ω–æ–≤) ‚Üí Redis –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "- **ParentDocumentRetriever** —Å–≤—è–∑—ã–≤–∞–µ—Ç chunks\n",
    "\n",
    "**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–æ–≤:**\n",
    "```bash\n",
    "uv add langchain-core langchain-community langchain-openai langchain-text-splitters langchain-qdrant\n",
    "uv add qdrant-client python-dotenv tiktoken tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ó–∞–≥—Ä—É–∑–∫–∞ .env –∏–∑: /home/anton-admin/–ó–∞–≥—Ä—É–∑–∫–∏/develop/education/agent_proj/lifelong_learning_assistant/rag/notebook/../.env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 11:15:47 | INFO | llm_service.llm_client | –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM-–∫–ª–∏–µ–Ω—Ç–∞: –ø—Ä–æ–≤–∞–π–¥–µ—Ä=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ LLMSettings\n",
      "   ‚Ä¢ –ü—Ä–æ–≤–∞–π–¥–µ—Ä: openai\n",
      "   ‚Ä¢ Chat Model: gpt-4o-mini\n",
      "   ‚Ä¢ Embedding Model: text-embedding-3-large\n",
      "   ‚Ä¢ API Base: https://api.ai-mediator.ru/v1\n",
      "   ‚Ä¢ Qdrant URL: http://localhost:6333\n",
      "   ‚Ä¢ Redis URL: redis://localhost:6379\n",
      "‚úÖ –ö–ª—é—á –≤ env –Ω–∞–π–¥–µ–Ω: sk-yZ...Ckxw\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ settings –∏ llm_service\n",
    "sys.path.append('..')\n",
    "\n",
    "# –Ø–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ .env –∏–∑ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–∏\n",
    "env_path = Path('..') / '.env'\n",
    "print(f\"üîç –ó–∞–≥—Ä—É–∑–∫–∞ .env –∏–∑: {env_path.absolute()}\")\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "from settings import get_settings\n",
    "from llm_service.llm_client import LLMClient\n",
    "\n",
    "cfg = get_settings()\n",
    "client = LLMClient(provider=\"openai\")\n",
    "\n",
    "print(\"üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —á–µ—Ä–µ–∑ LLMSettings\")\n",
    "print(f\"   ‚Ä¢ –ü—Ä–æ–≤–∞–π–¥–µ—Ä: {cfg.default_provider}\")\n",
    "print(f\"   ‚Ä¢ Chat Model: {cfg.openai_chat_model_name}\")\n",
    "print(f\"   ‚Ä¢ Embedding Model: {cfg.openai_embedding_model_name}\")\n",
    "print(f\"   ‚Ä¢ API Base: {cfg.openai_api_base}\")\n",
    "print(f\"   ‚Ä¢ Qdrant URL: {os.getenv('QDRANT_URL')}\")\n",
    "print(f\"   ‚Ä¢ Redis URL: {os.getenv('REDIS_URL')}\")\n",
    "\n",
    "# DEBUG: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–∞ (–≤—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)\n",
    "key = os.getenv('OPENAI_API_KEY')\n",
    "if key:\n",
    "    print(f\"‚úÖ –ö–ª—é—á –≤ env –Ω–∞–π–¥–µ–Ω: {key[:5]}...{key[-4:]}\")\n",
    "else:\n",
    "    print(\"‚ùå –ö–ª—é—á OPENAI_API_KEY –ù–ï –ù–ê–ô–î–ï–ù –≤ os.environ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ò–º–ø–æ—Ä—Ç—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton-admin/–ó–∞–≥—Ä—É–∑–∫–∏/develop/education/agent_proj/lifelong_learning_assistant/rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "from uuid import uuid4\n",
    "\n",
    "# LangChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.load import dumpd, loads\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
    "from langchain_community.storage import RedisStore\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams, SparseIndexParams\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"üì¶ –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –§—É–Ω–∫—Ü–∏–∏ –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document_text(text: str) -> str:\n",
    "    \"\"\"–û—á–∏—Å—Ç–∫–∞ Markdown-—Ç–µ–∫—Å—Ç–∞ –æ—Ç —Å—Å—ã–ª–æ–∫, —Ñ–∞–π–ª–æ–≤, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ [—Ç–µ–∫—Å—Ç](url) ‚Üí —Ç–µ–∫—Å—Ç\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ![alt](src)\n",
    "    text = re.sub(r'!\\[[^\\]]*\\].*?\\)', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ URL\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è **—Ç–µ–∫—Å—Ç** ‚Üí —Ç–µ–∫—Å—Ç\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*(.*?)\\*', r'\\1', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Lowercase –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_parent_id(meta: Dict[str, Any], text: str) -> str:\n",
    "    \"\"\"–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π parent_id\"\"\"\n",
    "    base = (\n",
    "        str(meta.get(\"source\", \"\")) + \"|\" +\n",
    "        str(meta.get(\"H1\", \"\")) + \"|\" + \n",
    "        str(meta.get(\"H2\", \"\")) + \"|\" + \n",
    "        str(meta.get(\"H3\", \"\")) + \"|\" +\n",
    "        (text[:128])\n",
    "    ).encode(\"utf-8\")\n",
    "    return hashlib.md5(base).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ë–ï–ó –æ—á–∏—Å—Ç–∫–∏)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑: ../../data/yandex-handbook-downloaded\n",
      "   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ 68 .md —Ñ–∞–π–ª–æ–≤\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 68 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tiktoken_len = lambda text: len(encoder.encode(text))\n",
    "\n",
    "# –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º\n",
    "DATA_PATH = \"../../data/yandex-handbook-downloaded\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ë–ï–ó –æ—á–∏—Å—Ç–∫–∏ (–Ω—É–∂–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–∏ ##, ###)\n",
    "print(f\"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑: {DATA_PATH}\")\n",
    "md_files = list(Path(DATA_PATH).glob(\"*.md\"))\n",
    "print(f\"   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ {len(md_files)} .md —Ñ–∞–π–ª–æ–≤\")\n",
    "\n",
    "documents = []\n",
    "for file_path in md_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            'source': str(file_path),\n",
    "            'filename': file_path.name,\n",
    "            'total_tokens': tiktoken_len(content)\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –ó–∞–≥—Ä—É–∑–∫–∞ index.json –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 68 –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º index.json\n",
    "INDEX_PATH = Path(DATA_PATH) / \"index.json\"\n",
    "\n",
    "with open(INDEX_PATH, 'r', encoding='utf-8') as f:\n",
    "    index_data = json.load(f)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ filename -> title\n",
    "filename_to_title = {}\n",
    "for doc_id, doc_info in index_data.items():\n",
    "    filename = doc_id + '.md'\n",
    "    title = doc_info.get('title', '')\n",
    "    filename_to_title[filename] = title\n",
    "\n",
    "print(f\"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filename_to_title)} –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–ª–∏—Ç—Ç–µ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown Header Splitter –¥–ª—è parent chunks\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"H1\"),\n",
    "    (\"##\", \"H2\"),\n",
    "    (\"###\", \"H3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "# Recursive splitter –¥–ª—è –±–æ–ª—å—à–∏—Ö parent chunks\n",
    "parent_recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# Child splitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450,\n",
    "    chunk_overlap=70,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. –°–æ–∑–¥–∞–Ω–∏–µ parent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–∏–µ parent chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68/68 [00:00<00:00, 87.12it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ–∑–¥–∞–Ω–æ 923 parent chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parent_docs = []\n",
    "for doc in tqdm(documents, desc=\"–°–æ–∑–¥–∞–Ω–∏–µ parent chunks\"):\n",
    "    doc_filename = doc.metadata.get('filename', '')\n",
    "    doc_title = filename_to_title.get(doc_filename, '')\n",
    "    \n",
    "    md_chunks = markdown_splitter.split_text(doc.page_content or \"\")\n",
    "    \n",
    "    for chunk in md_chunks:\n",
    "        merged_meta = {**(doc.metadata or {}), **(chunk.metadata or {})}\n",
    "        merged_meta[\"doc_title\"] = doc_title\n",
    "        \n",
    "        headers = [merged_meta.get(k, \"\") for k in (\"H1\", \"H2\", \"H3\") if k in merged_meta]\n",
    "        merged_meta[\"breadcrumbs\"] = \" / \".join(headers) if headers else \"\"\n",
    "        \n",
    "        path_parts = [doc_title] if doc_title else []\n",
    "        path_parts.extend(headers)\n",
    "        merged_meta[\"path\"] = \" / \".join(path_parts) if path_parts else \"\"\n",
    "        \n",
    "        merged_meta[\"level\"] = 3 if \"H3\" in merged_meta else 2 if \"H2\" in merged_meta else 1 if \"H1\" in merged_meta else 0\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç chunk\n",
    "        chunk.page_content = clean_document_text(chunk.page_content)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º parent_id\n",
    "        merged_meta[\"parent_id\"] = stable_parent_id(merged_meta, chunk.page_content)\n",
    "        chunk.metadata = merged_meta\n",
    "        \n",
    "        if tiktoken_len(chunk.page_content) > 1500:\n",
    "            sub_chunks = parent_recursive_splitter.split_documents([chunk])\n",
    "            for sub in sub_chunks:\n",
    "                sub.metadata = merged_meta.copy()\n",
    "                sub.metadata[\"parent_id\"] = stable_parent_id(sub.metadata, sub.page_content)\n",
    "            parent_docs.extend(sub_chunks)\n",
    "        else:\n",
    "            parent_docs.append(chunk)\n",
    "\n",
    "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(parent_docs)} parent chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Qdrant –∏ Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-15 11:16:18 | INFO | llm_service.llm_client | start:create_embeddings –ø—Ä–æ–≤–∞–π–¥–µ—Ä=openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ú® –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (—á–µ—Ä–µ–∑ LLMClient)\n",
      "üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è yandex_handbook_child_chunks\n",
      "üìè –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: 3072\n",
      "‚úÖ Qdrant –Ω–∞—Å—Ç—Ä–æ–µ–Ω: yandex_handbook_child_chunks\n",
      "‚úÖ Redis –Ω–∞—Å—Ç—Ä–æ–µ–Ω: redis://localhost:6379\n"
     ]
    }
   ],
   "source": [
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º LLMClient –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "embeddings = client.create_embeddings()\n",
    "\n",
    "# Sparse embeddings (BM25)\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "print(\"‚ú® –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (—á–µ—Ä–µ–∑ LLMClient)\")\n",
    "\n",
    "# Qdrant\n",
    "qdrant_url = os.getenv('QDRANT_URL', 'http://localhost:6333')\n",
    "collection_name = os.getenv('QDRANT_COLLECTION_NAME', 'yandex_handbook_child_chunks')\n",
    "qdrant_client = QdrantClient(url=qdrant_url)\n",
    "\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(f\"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {collection_name}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ –º–æ–¥–µ–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "sample_vector = embeddings.embed_query(\"test\")\n",
    "vector_size = len(sample_vector)\n",
    "print(f\"üìè –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {vector_size}\")\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"dense\": VectorParams(size=vector_size, distance=Distance.COSINE)\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": SparseVectorParams(\n",
    "            index=SparseIndexParams(on_disk=False),\n",
    "            modifier=models.Modifier.IDF,\n",
    "        )\n",
    "    },\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    vector_name=\"dense\",\n",
    "    sparse_vector_name=\"sparse\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Qdrant –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {collection_name}\")\n",
    "\n",
    "# Redis\n",
    "redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379')\n",
    "parent_store = RedisStore(\n",
    "    redis_url=redis_url,\n",
    "    namespace=\"rag:parents\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Redis –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {redis_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Redis: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 923/923 [00:00<00:00, 9918.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 923 parent chunks –≤ Redis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–°–æ–∑–¥–∞–Ω–∏–µ child chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 923/923 [00:01<00:00, 576.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ–∑–¥–∞–Ω–æ 2071 child chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:27<00:00,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä –ò—Ç–æ–≥–æ: Qdrant=2071, Redis=923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
    "\n",
    "def doc_to_bytes(doc: Document) -> bytes:\n",
    "    as_dict = dumpd(doc)\n",
    "    return json.dumps(as_dict, ensure_ascii=False).encode(\"utf-8\")\n",
    "\n",
    "# 1. Redis\n",
    "parent_ids = []\n",
    "batch = []\n",
    "for doc in tqdm(parent_docs, desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Redis\"):\n",
    "    pid = doc.metadata[\"parent_id\"]\n",
    "    batch.append((pid, doc_to_bytes(doc)))\n",
    "    parent_ids.append(pid)\n",
    "    if len(batch) >= 1000:\n",
    "        parent_store.mset(batch)\n",
    "        batch.clear()\n",
    "if batch: parent_store.mset(batch)\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(parent_ids)} parent chunks –≤ Redis\")\n",
    "\n",
    "# 2. Child chunks -> Qdrant\n",
    "child_docs = []\n",
    "for parent in tqdm(parent_docs, desc=\"–°–æ–∑–¥–∞–Ω–∏–µ child chunks\"):\n",
    "    parent_id = parent.metadata[\"parent_id\"]\n",
    "    children = child_splitter.split_documents([parent])\n",
    "    for child in children:\n",
    "        child.metadata.update(parent.metadata)\n",
    "        child.metadata[\"parent_id\"] = parent_id\n",
    "        child.metadata[\"child_id\"] = str(uuid4())\n",
    "        child.metadata[\"is_child_chunk\"] = True\n",
    "        path = child.metadata.get(\"path\", \"\")\n",
    "        if path: child.page_content = f\"{path}\\n\\n{child.page_content}\"\n",
    "        child_docs.append(child)\n",
    "\n",
    "print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(child_docs)} child chunks\")\n",
    "\n",
    "if child_docs:\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(child_docs), batch_size), desc=\"–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant\"):\n",
    "        batch = child_docs[i:i+batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "\n",
    "collection_info = qdrant_client.get_collection(collection_name)\n",
    "print(f\"\\nüìä –ò—Ç–æ–≥–æ: Qdrant={collection_info.points_count}, Redis={len(parent_ids)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-llm-cource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
