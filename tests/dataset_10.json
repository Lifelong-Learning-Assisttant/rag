[
    {
      "question": "Какой обзор является основным источником материала в документе, кроме части про AdaGrad?",
      "answer": "Обзор Х. Брендана МакМахана под названием «A Survey of Algorithms and Analysis for Adaptive Online Learning».",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Какие два основных типа регуляризаторов выделяются в классе алгоритмов FTRL?",
      "answer": "Центрированные регуляризаторы, минимум которых находится в фиксированной точке, и проксимальные регуляризаторы, минимум которых находится в текущей точке на каждом шаге.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Как называется ключевая лемма для оценки качества работы FTRL и в чём её смысл?",
      "answer": "Strong FTRL Lemma. Она помогает оценить верхнюю границу regret как сумму двух частей: регуляризации в оптимальной точке и меры стабильности алгоритма — насколько мало меняются решения от шага к шагу.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Почему на практике удобно использовать диагональные матрицы в адаптивных регуляризаторах вместо произвольных?",
      "answer": "Потому что диагональные матрицы требуют гораздо меньше памяти и позволяют обновлять параметры очень быстро — поэлементно, без решения больших систем уравнений.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Зачем вводят квадратичные регуляризаторы при построении адаптивного FTRL?",
      "answer": "Они упрощают анализ: для них легко подобрать подходящую норму, легко вычисляется двойственная норма, и удобно получать оценки качества работы алгоритма.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Как связан алгоритм FTRL-Proximal со стохастическим градиентным спуском с убывающим шагом обучения?",
      "answer": "FTRL-Proximal с определённым выбором регуляризатора полностью эквивалентен стохастическому градиентному спуску, в котором шаг обучения уменьшается обратно пропорционально сумме коэффициентов регуляризации.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Как определяется размер шага в AdaGrad для отдельной координаты на каждом шаге?",
      "answer": "Он обратно пропорционален квадратному корню из суммы квадратов градиентов по этой координате на всех предыдущих шагах.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Какая статья стала основой для AdaGrad и получила престижную премию Test of Time Award на NeurIPS в 2021 году?",
      "answer": "Статья «Regularized Dual Averaging» от Линь Сяо.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Чем отличаются оценки regret для общего FTRL и для FTRL-Proximal?",
      "answer": "Они отличаются тем, включают ли регуляризаторы шага t в сумму: в Proximal-варианте включают, в общей версии — нет. Это небольшое техническое различие, но оно делает Proximal-подход удобнее.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
      "question": "Почему при выводе AdaGrad минимизируют вторую часть оценки regret, а не первую?",
      "answer": "Потому что первая часть зависит от неизвестного заранее оптимального решения, а вторая часть зависит только от того, что уже произошло — от предыдущих точек и градиентов, и её можно контролировать напрямую.",
      "doc": "adaptivnyj-ftrl.md"
    },
    {
        "question": "Что показывает автокорреляционная функция временного ряда?",
        "answer": "Она показывает, насколько значения ряда в разные моменты времени зависят друг от друга — например, насколько сегодняшнее значение связано со вчерашним, позавчерашним и так далее.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Как понять, значима ли автокорреляция при заданном лаге?",
        "answer": "Нужно посмотреть p-value критерия Льюнга-Бокса: если оно меньше заранее выбранного порога значимости (например, 0.05), то автокорреляция считается статистически значимой.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Какие признаки можно использовать для прогноза, если автокорреляция значима при лагах 1, 2 и 12?",
        "answer": "Следует использовать значения ряда с лагами 1, 2 и 12 — то есть значения за 1, 2 и 12 шагов назад (например, за вчера, позавчера и год назад, если шаг — месяц).",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Что означает стационарность временного ряда в широком смысле?",
        "answer": "Это означает, что среднее значение ряда не меняется со временем, а автокорреляция зависит только от расстояния между моментами, а не от самих моментов.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Почему случайное блуждание не является стационарным рядом?",
        "answer": "Потому что его дисперсия растёт со временем — ряд всё сильнее «разбегается», несмотря на то, что среднее может оставаться постоянным.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Какие два популярных статистических критерия применяют для проверки стационарности ряда?",
        "answer": "Критерий Дики-Фуллера и критерий KPSS.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Как можно убрать тренд из временного ряда?",
        "answer": "Через дифференцирование — то есть переход к разностям между соседними значениями ряда.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Какие два основных типа сезонности можно наблюдать в ряде потребления электроэнергии в Австралии?",
        "answer": "Суточная и недельная сезонности — например, пики потребления в вечернее время и в будние дни.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Что происходит со сглаженным рядом при увеличении параметра сглаживания в простом экспоненциальном сглаживании?",
        "answer": "При увеличении параметра сглаживания ряд становится менее сглаженным и сильнее следует за исходными данными, но при этом сохраняет больше шума.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Чем отличается адаптивное экспоненциальное сглаживание от обычного?",
        "answer": "В адаптивном сглаживании параметр сглаживания не фиксирован, а меняется во времени в зависимости от ошибок прогноза — например, увеличивается при резких изменениях в поведении ряда.",
        "doc": "analitika-vremennyh-ryadov.md"
    },
    {
        "question": "Что такое бэггинг и как он уменьшает ошибку модели?",
        "answer": "Бэггинг — это метод, при котором создаётся множество моделей на случайных подвыборках обучающих данных, а затем их предсказания усредняются. Он снижает разброс (дисперсию) ансамбля, не увеличивая смещение.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Что такое разложение ошибки на смещение и разброс?",
        "answer": "Это способ анализа ошибки модели: общая ошибка складывается из неустранимого шума, смещения (систематического отклонения среднего предсказания от истинного значения) и разброса (изменчивости предсказаний в зависимости от обучающей выборки).",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Как в случайном лесе уменьшается корреляция между деревьями?",
        "answer": "В каждой вершине при построении дерева случайно выбирается подмножество признаков (не все), и сплит ищется только среди них — это делает деревья менее похожими друг на друга.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Почему в случайном лесу рекомендуется использовать глубокие деревья?",
        "answer": "Потому что глубокие деревья имеют низкое смещение, а бэггинг уже снижает разброс — вместе это даёт ансамбль с низким и смещением, и разбросом.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Как рекомендуется выбирать количество признаков для одного дерева в случайном лесе?",
        "answer": "Для задач классификации — примерно корень из общего числа признаков, для регрессии — около трети от общего числа.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Чем бустинг принципиально отличается от бэггинга?",
        "answer": "В бустинге базовые модели обучаются последовательно, и каждая следующая модель пытается исправить ошибки предыдущих, тогда как в бэггинге все модели обучаются независимо и параллельно.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Какую цель преследует бустинг в терминах смещения и разброса?",
        "answer": "Бустинг в первую очередь снижает смещение итоговой модели — поэтому в нём используют простые, слабо смещающиеся, но быстро обучающиеся базовые модели.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Почему в бустинге обычно используют неглубокие деревья?",
        "answer": "Потому что глубокие деревья имеют низкое смещение, но высокий разброс и долго обучаются. В бустинге важна скорость и высокое исходное смещение — чтобы была возможность его уменьшать последовательными шагами.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Чем стекинг отличается от бэггинга и бустинга?",
        "answer": "В стекинге можно использовать разные типы моделей (не только один и тот же), а их предсказания объединяются не простым усреднением, а с помощью обучаемой мета-модели.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Зачем в стекинге используют кросс-валидационное построение мета-признаков на обучающей выборке?",
        "answer": "Чтобы избежать переобучения мета-модели: предсказания базовых моделей для обучения мета-уровня делаются на тех данных, на которых эти модели не обучались — аналогично кросс-валидации.",
        "doc": "ansambli-v-mashinnom-obuchenii.md"
    },
    {
        "question": "Как в байесовском подходе выражается идея «веса модели не должны быть слишком большими»?",
        "answer": "Через выбор априорного распределения на веса — например, нормального распределения с нулевым средним, в котором маленькие значения весов считаются более вероятными, чем большие.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Что такое апостериорное распределение и как оно связано с априорным?",
        "answer": "Апостериорное распределение — это обновлённое представление о параметрах модели после учёта данных. Оно получается из априорного распределения с помощью формулы Байеса.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Почему в байесовском подходе удобно использовать сопряжённые распределения?",
        "answer": "Потому что при их использовании апостериорное распределение оказывается из того же семейства, что и априорное, и его легко вычислить аналитически.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Что такое MAP-оценка и чем она отличается от MLE-оценки?",
        "answer": "MAP-оценка — это наиболее вероятное значение параметра с учётом и данных, и априорных знаний. MLE-оценка игнорирует априорные знания и учитывает только данные.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Как L2-регуляризация в линейной регрессии связана с байесовским подходом?",
        "answer": "Она эквивалентна наложению нормального априорного распределения на веса модели с нулевым средним.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Как L1-регуляризация в линейной регрессии связана с байесовским подходом?",
        "answer": "Она эквивалентна наложению лапласовского априорного распределения на веса — в нём также предпочтение отдаётся малым по модулю значениям, но с более «острым» пиком в нуле.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Как делается предсказание для нового объекта в байесовской модели?",
        "answer": "Можно либо использовать точечную оценку параметров (например, MAP), либо усреднить предсказания по всем возможным значениям параметров с учётом их апостериорной вероятности — это даёт не одно число, а распределение предсказания.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Как байесовский подход позволяет дообучать модель при поступлении новых данных?",
        "answer": "Апостериорное распределение после обработки старых данных становится новым априорным, и его легко обновить с помощью формулы Байеса при поступлении новых наблюдений — без полного пересчёта.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Что такое обоснованность модели (marginal likelihood) и как она помогает выбирать модель?",
        "answer": "Обоснованность — это вероятность данных при данной модели, усреднённая по всем возможным параметрам. Она автоматически штрафует слишком сложные модели и помогает найти баланс между точностью и простотой.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Чем байесовский подход к оценке неопределённости отличается от фреквентистского (например, через асимптотическую нормальность MLE)?",
        "answer": "Байесовский подход прямо моделирует неопределённость в параметрах как распределение, обновляемое данными. Фреквентистский подход интерпретирует параметры как фиксированные, а неопределённость возникает только из-за случайности выборки — и для практических оценок требует сильных приближений.",
        "doc": "bajesovskij-podhod-k-ocenivaniyu.md"
      },
      {
        "question": "Что такое разложение ошибки на смещение и разброс?",
        "answer": "Это способ анализа ошибки модели, при котором общая ошибка делится на три части: смещение (систематическая ошибка, насколько в среднем модель ошибается), разброс (насколько сильно предсказания модели меняются при изменении обучающих данных) и неустранимый шум (случайные колебания в данных).",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Как изменяются смещение и разброс при увеличении глубины решающего дерева?",
        "answer": "Смещение уменьшается — дерево лучше подстраивается под истинную зависимость. Разброс растёт — модель сильнее реагирует на изменения в обучающей выборке.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Как визуально можно представить высокое смещение и низкий разброс?",
        "answer": "Как множество предсказаний (например, синих точек), которые все близки друг к другу, но в среднем находятся далеко от истинного значения (цели).",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Для каких функций потерь точно верно классическое разложение на смещение и разброс?",
        "answer": "Только для квадратичной функции потерь (MSE). Для других функций потерь существуют обобщённые формы разложения, но они могут выглядеть иначе.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Что такое bias-variance trade-off в классическом понимании?",
        "answer": "Это идея о том, что при увеличении сложности модели смещение уменьшается, а разброс растёт, и поэтому общая ошибка сначала падает, а потом начинает расти, образуя U-образную кривую.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Всегда ли рост сложности модели приводит к росту разброса?",
        "answer": "Нет. В современных моделях, например, в очень больших нейронных сетях или глубоких ансамблях деревьев, при достаточном количестве параметров могут одновременно снижаться и смещение, и разброс — это явление называют «двойным спуском» (double descent).",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Почему глубокие деревья могут потерять способность к обобщению?",
        "answer": "Потому что при слишком большой глубине дерево начинает идеально подстраиваться под зашумлённые обучающие данные, включая шум как часть сигнала — и поэтому плохо работает на новых данных.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Какие источники случайности учитываются при вычислении общей ошибки в разложении bias-variance?",
        "answer": "Случайность в обучающей выборке (разные наборы данных приводят к разным моделям), случайность в шуме целевой переменной (разные реализации шума при одном и том же объекте) и случайность в выборе тестового объекта.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Какую роль играет шум в данных в разложении ошибки?",
        "answer": "Шум — это неустранимая часть ошибки, которая не зависит ни от модели, ни от алгоритма, и которую нельзя уменьшить никакими методами обучения.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Можно ли наблюдать классический bias-variance trade-off в методе k ближайших соседей?",
        "answer": "Да, но «наоборот» по оси сложности: чем больше k (чем проще модель), тем выше смещение и ниже разброс; чем меньше k (чем сложнее модель), тем ниже смещение и выше разброс.",
        "doc": "bias-variance-decomposition.md"
      },
      {
        "question": "Как в общих чертах работает диффузионная модель при генерации объекта?",
        "answer": "Она начинает с чистого шума и постепенно удаляет шум шаг за шагом, пока не получит осмысленный объект — например, изображение — похожее на те, что были в обучающей выборке.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Что такое прямой диффузионный процесс?",
        "answer": "Это процесс последовательного добавления гауссовского шума к исходному изображению по заданному расписанию, в результате которого изображение превращается в чистый шум.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Что такое обратный диффузионный процесс?",
        "answer": "Это процесс, противоположный прямому: он начинается с шума и постепенно восстанавливает изображение, убирая шум шаг за шагом с помощью обученной модели.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Что предсказывает модель в самой популярной версии диффузионного обучения?",
        "answer": "Она предсказывает, какой шум был добавлен к зашумлённому изображению на текущем шаге — и на основе этого шум удаляется.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Почему сэмплирование (генерация) в диффузионных моделях изначально было очень медленным?",
        "answer": "Потому что для получения одного изображения требовалось выполнить сотни шагов обратного процесса — по одному за итерацию — что занимало часы на GPU.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Чем косинусное расписание шума лучше линейного?",
        "answer": "Линейное расписание плохо работает на маленьких изображениях: последние шаги прямого процесса просто зашумляют уже почти чистый шум, то есть не несут полезной информации. Косинусное расписание избегает этого, делая зашумление более равномерным по информативности.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Что такое classifier guidance и зачем он нужен?",
        "answer": "Это метод условной генерации, при котором во время сэмплирования используется предобученный классификатор на зашумлённых изображениях, чтобы направлять генерацию в сторону нужного класса — например, «кошка» или «собака».",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "В чём преимущество classifier-free guidance перед classifier guidance?",
        "answer": "Classifier-free guidance не требует отдельно обучать классификатор: условная и безусловная модели обучаются одновременно, а во время генерации используется только одна модель.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Какие основные недостатки были у первых диффузионных моделей (DDPM)?",
        "answer": "Они были крайне медленными при генерации изображений и требовали большого числа шагов; также линейное расписание шума давало неоптимальные результаты на маленьких изображениях.",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Можно ли применять диффузионные модели к дискретным данным, например, к тексту?",
        "answer": "Да, но вместо нормальных распределений используются категориальные, а вместо добавления шума — переходы по матрицам вероятностей, которые постепенно приводят к равномерному распределению (аналогу шума).",
        "doc": "diffuzionnye-modeli.md"
      },
      {
        "question": "Что такое дистилляция знаний и какова её основная цель?",
        "answer": "Дистилляция знаний — это метод, при котором знания из большой и сложной модели (учителя) передаются маленькой модели (ученику), чтобы сохранить высокое качество предсказаний при меньшем размере и быстром применении.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Какие «теневые знания» передаются от учителя к ученику в хинтоновской дистилляции?",
        "answer": "Это информация о вероятностях принадлежности объекта ко *всем* классам, а не только к правильному. Такие вероятности отражают сходства между классами и позволяют ученику учиться тонким взаимосвязям в данных.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Зачем в дистилляции знаний используется параметр температуры?",
        "answer": "Он сглаживает распределение вероятностей учителя — делает его менее «уверенным» и более равномерным, чтобы ученик мог лучше уловить тонкие различия между классами, а не просто копировать жёсткий максимум.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Почему DistilBERT инициализируют весами каждого второго слоя BERT?",
        "answer": "Потому что DistilBERT вдвое короче BERT, и прямое копирование весов соответствующих слоёв даёт хорошую стартовую точку, что ускоряет обучение и повышает итоговое качество — без этого теряется около 1% точности.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Чем online-дистилляция отличается от offline-дистилляции?",
        "answer": "В offline-дистилляции учитель уже обучен и не меняется; в online-дистилляции ученик и учитель — две одинаковые модели, которые обучаются одновременно и обмениваются знаниями в процессе.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Как работает самодистилляция при обучении одной и той же архитектуры?",
        "answer": "Сначала обучают одну модель обычным способом, затем используют её предсказания как «мягкую разметку» для обучения второй модели той же архитектуры с новой инициализацией — и это часто улучшает обобщающую способность.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Какие слои кроме выходного можно использовать при дистилляции и зачем?",
        "answer": "Можно использовать промежуточные слои — их выходы (внутренние представления) тоже содержат полезную информацию. Дистилляция таких слоёв помогает ученику воспроизводить не только финальный ответ, но и логику рассуждений учителя.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Как дистилляция помогает при квантизации моделей?",
        "answer": "После перевода весов в низкую точность (например, float16 → int8) модель дообучают, используя предсказания оригинальной высокоточной модели — это компенсирует падение качества из-за потери точности.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Как дистилляция применяется в методе BYOL для self-supervised обучения?",
        "answer": "В BYOL одна модель (online) обучается копировать внутренние представления другой модели (target), которая постоянно обновляется как сглаженная копия первой. Это предотвращает коллапс представлений и позволяет учиться без разметки.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Почему иногда дистилляция из более точной модели даёт *худшего* ученика?",
        "answer": "Пока нет исчерпывающего объяснения, но предполагается, что слишком сложный учитель может давать слишком «тонкие» сигналы, которые маленькая модель не в состоянии понять — или даже переобучиться на шум в её предсказаниях.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "В чём основная идея метода моментов?",
        "answer": "Он заключается в том, чтобы использовать выборочные моменты (например, среднее и дисперсию) для составления уравнений, связывающих их с теоретическими моментами распределения, и решить эти уравнения, чтобы оценить параметры модели.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Что измеряет энтропия Шеннона?",
        "answer": "Она характеризует степень неопределённости или «незнания» о случайной величине: чем выше энтропия, тем сложнее предсказать значение, и тем больше информации в среднем нужно, чтобы его передать.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Почему биномиальное распределение с вероятностью успеха 0.5 имеет максимальную энтропию?",
        "answer": "Потому что в этом случае исходы «успех» и «неудача» равновероятны, и предсказать результат одного испытания невозможно — это соответствует наибольшей неопределённости.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Как интерпретируется дивергенция Кульбака-Лейблера между двумя распределениями?",
        "answer": "Она показывает, насколько увеличится средняя длина кода при использовании алгоритма, оптимизированного под одно распределение, для кодирования данных, на самом деле сгенерированных из другого.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Что гласит принцип максимальной энтропии?",
        "answer": "Среди всех распределений, удовлетворяющих заданным ограничениям (например, фиксированное среднее), следует выбирать то, у которого энтропия максимальна — оно содержит минимум дополнительных предположений.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Какое распределение имеет максимальную энтропию среди всех непрерывных распределений с заданными средним и дисперсией?",
        "answer": "Нормальное распределение.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Почему равномерное распределение на отрезке не относится к экспоненциальному классу?",
        "answer": "Потому что его носитель (отрезок) зависит от параметров — в экспоненциальном классе носитель должен быть фиксированным для всего семейства, а параметры могут влиять только на форму через экспоненту.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Как связаны метод максимального правдоподобия и метод моментов для распределений из экспоненциального класса?",
        "answer": "Для таких распределений они совпадают: оценка максимального правдоподобия параметров эквивалентна приравниванию теоретических и выборочных значений определённых функций (аналогов моментов).",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Какие распределения входят в экспоненциальный класс?",
        "answer": "Нормальное, биномиальное, пуассоновское, экспоненциальное, гамма, бета, геометрическое и многие другие; не входят — равномерное на отрезке, распределение Коши, смеси нормальных.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Всегда ли существует распределение с максимальной энтропией при заданных ограничениях?",
        "answer": "Нет. Например, если задать только среднее значение на всей числовой прямой, то энтропия можно сделать сколь угодно большой (например, беря нормальные распределения с всё большей дисперсией), и максимума не существует.",
        "doc": "eksponencialnyj-klass-raspredelenij-i-princip-maksimalnoj-entropii.md"
      },
      {
        "question": "Как энтропия связана с информативностью наблюдения?",
        "answer": "Чем выше вероятность наблюдения, тем менее информативно оно — и наоборот: редкие, маловероятные события несут больше информации. Энтропия — это средняя информативность одного наблюдения.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Почему энтропия схемы Бернулли максимальна при вероятности успеха 0.5?",
        "answer": "Потому что в этом случае исход эксперимента наиболее неопределён: ни успех, ни неудача не преобладают, и предсказать результат сложнее всего.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Чем дифференциальная энтропия отличается от энтропии дискретного распределения?",
        "answer": "Она может быть отрицательной, потому что плотность распределения может превышать единицу. Также она не всегда имеет прямую интерпретацию как среднее число бит для кодирования — в отличие от дискретной энтропии.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Что измеряет дивергенция Кульбака-Лейблера между двумя распределениями?",
        "answer": "Она показывает, насколько возрастёт средняя длина кода, если данные из одного распределения кодировать оптимальным кодом, построенным для другого распределения.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Почему KL-дивергенция не является симметричной?",
        "answer": "Потому что расстояние от распределения A до B измеряет рост затрат при использовании кода, оптимизированного под B, для кодирования данных из A — и это не то же самое, что наоборот.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Как кросс-энтропия используется в задаче классификации?",
        "answer": "В качестве функции потерь: она измеряет разницу между предсказанным моделью распределением вероятностей по классам и истинным (one-hot) распределением. Минимизация кросс-энтропии — это стремление сделать предсказание как можно ближе к истине.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "В чём суть принципа максимальной энтропии при выборе модели?",
        "answer": "Среди всех распределений, удовлетворяющих заданным ограничениям (например, среднее и дисперсия), выбирается то, у которого энтропия максимальна — то есть то, которое добавляет минимум предположений сверх этих ограничений.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Почему нормальное распределение часто «побеждает» по энтропии при фиксированных среднем и дисперсии?",
        "answer": "Потому что KL-дивергенция от любого такого распределения до нормального всегда неотрицательна, и обнуляется только при их совпадении — значит, энтропия нормального распределения максимально возможна в данном классе.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Какие распределения входят в экспоненциальный класс?",
        "answer": "Нормальное, биномиальное, пуассоновское, экспоненциальное, гамма, бета, геометрическое и многие другие. Не входят — равномерное с переменными границами, распределение Коши, смеси нормальных.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Почему метод множителей Лагранжа естественно приводит к экспоненциальному классу при максимизации энтропии?",
        "answer": "При дифференцировании лагранжиана по вероятностям получается условие, что логарифмы вероятностей должны быть линейными комбинациями наложенных ограничений — а это как раз означает, что само распределение имеет экспоненциальную форму.",
        "doc": "entropiya-i-semejstvo-eksponencialnyh-raspredelenij.md"
      },
      {
        "question": "Какова основная идея работы генеративно-состязательных сетей (GAN)?",
        "answer": "GAN состоит из двух нейросетей — генератора и дискриминатора, которые обучаются одновременно: генератор создаёт поддельные объекты (например, изображения), а дискриминатор пытается отличать их от настоящих. В процессе обучения они «состязаются»: генератор стремится обмануть дискриминатор, а тот — стать лучше в распознавании подделок.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Что такое mode collapse и почему это проблема для GAN?",
        "answer": "Mode collapse — это ситуация, когда генератор выдаёт разнообразные, но реалистичные изображения только из ограниченного подмножества возможных вариантов (например, только кошек, хотя обучался и на собаках), игнорируя редкие классы или даже генерируя почти одинаковые картинки. Это снижает разнообразие и полезность модели.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Что измеряет метрика Frechet Inception Distance (FID)?",
        "answer": "FID оценивает, насколько близко распределение признаков, извлечённых из сгенерированных изображений, к распределению признаков реальных изображений. Для этого она сравнивает гауссовые приближения этих распределений, используя статистики (среднее и ковариацию) глубоких признаков из предобученной сети Inception v3.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Зачем в DCGAN используют операцию nearest upsampling?",
        "answer": "Nearest upsampling увеличивает пространственное разрешение карт признаков внутри генератора — это необходимо, потому что генератор строит изображение постепенно, начиная с маленького тензора и «наращивая» его до нужного размера, в отличие от дискриминатора, который наоборот уменьшает разрешение.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Как работает conditional GAN (cGAN)?",
        "answer": "В cGAN условие (например, метка класса или текстовое описание) подаётся и в генератор, и в дискриминатор — обычно через конкатенацию с вектором шума (в генераторе) или с входным изображением (в дискриминаторе), чтобы обе сети учились генерировать и различать объекты с учётом условия.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Что такое truncation trick в StyleGAN и зачем он нужен?",
        "answer": "Truncation trick — это приём, при котором после обучения генератора векторы из латентного пространства сдвигаются ближе к их среднему значению. Это повышает качество (реализм) генерируемых изображений за счёт некоторой потери разнообразия — плохие, «выбросные» примеры фильтруются.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Почему адаптивная инстанс-нормализация (AdaIN) важна для StyleGAN?",
        "answer": "AdaIN позволяет «подмешивать» информацию из вектора шума (стиля) на разных уровнях генератора, а не только в самом начале. Это даёт генератору возможность использовать глобальные признаки (например, позу лица) на ранних этапах и локальные детали (текстуру кожи) — на поздних, улучшая контроль и качество.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Что изменили в StyleGAN2 по сравнению с оригинальным StyleGAN для устранения артефактов?",
        "answer": "В StyleGAN2 отказались от вычитания среднего в нормализации (оставили только масштабирование), перенесли нормализацию и модуляцию в начало и конец блоков, а также заменили нормализацию входов на демодуляцию весов свёрток. Кроме того, вместо пошагового увеличения разрешения (progressive growing) стали использовать остаточные связи (residual connections).",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Что такое ADA в StyleGAN-ADA и как оно работает?",
        "answer": "ADA означает Adaptive Discriminator Augmentation — адаптивное применение аугментаций (например, поворотов, обрезок) к изображениям, подаваемым на вход дискриминатору. Степень аугментации автоматически регулируется в ходе обучения: если дискриминатор начинает переобучаться, аугментации усиливаются, и наоборот.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "В чём главное преимущество StyleGAN-T перед диффузионными моделями, несмотря на более низкое качество генерации?",
        "answer": "Главное преимущество — скорость генерации: StyleGAN-T создаёт изображение примерно за 0.02 секунды, тогда как Stable Diffusion тратит на это около 3.7 секунд на той же видеокарте.",
        "doc": "generativno-sostyazatelnye-seti-(gan).md"
      },
      {
        "question": "Чем принципиально отличается генеративный подход от дискриминативного в задаче классификации?",
        "answer": "Дискриминативные модели напрямую оценивают вероятность принадлежности объекта к классу по его признакам. Генеративные же модели оценивают, как распределены признаки внутри каждого класса, и на основе этого — каковы вероятности классов.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Какое ключевое преимущество даёт генеративная модель в сравнении с дискриминативной?",
        "answer": "Она способна обнаруживать выбросы — объекты, которые плохо соответствуют ни одному из известных классов, потому что оценивает, насколько правдоподобен сам объект в каждом классе.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Как генеративная модель может быть полезна в задаче распознавания речи (ASR), даже если размеченных аудиозаписей мало?",
        "answer": "Она позволяет использовать языковую модель, обученную на большом корпусе текстов без аудио, чтобы оценивать правдоподобие распознанного предложения — и выбрать из нескольких фонетически похожих вариантов тот, который чаще встречается в языке.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Чем отличается линейный дискриминантный анализ (LDA) от квадратичного (GDA)?",
        "answer": "В LDA предполагается, что ковариационные матрицы всех классов одинаковы, из-за чего разделяющая поверхность становится линейной. В GDA матрицы у каждого класса свои, и граница — квадратичная поверхность.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Почему LDA считается менее склонной к переобучению, чем GDA?",
        "answer": "Потому что LDA оценивает одну общую для всех классов ковариационную матрицу, а не отдельную для каждого класса. Это сильно уменьшает число параметров, особенно когда признаков много.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Какую гипотезу делает наивный байесовский классификатор и почему он «наивный»?",
        "answer": "Он предполагает, что признаки объекта независимы друг от друга внутри каждого класса. На практике такое почти никогда не выполняется — признаки часто коррелированы, поэтому подход и называется «наивным».",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Что такое сглаживание Лапласа и зачем оно нужно в наивном байесе?",
        "answer": "Это приём, при котором к числителю и знаменателю частотной оценки вероятности добавляются маленькие константы. Оно нужно, чтобы избежать нулевых вероятностей для редких или не встретившихся в обучении значений признаков.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Как можно оценить плотность непрерывного распределения без предположений о его форме?",
        "answer": "С помощью непараметрической оценки ядерной плотности: для каждой точки считаем взвешенную сумму объектов из выборки, где вес тем выше, чем ближе объект к этой точке.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Как связаны наивный байес с нормальными распределениями и логистическая регрессия?",
        "answer": "Если в наивном байесе признаки распределены нормально и имеют одинаковую дисперсию в обоих классах, то полученная формула для вероятности класса совпадает с формулой логистической регрессии. Но сама логистическая регрессия не требует этих сильных предположений.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "Почему гауссовский наивный байес можно рассматривать как частный случай GDA?",
        "answer": "Потому что в гауссовском наивном байесе ковариационные матрицы диагональные — то есть признаки независимы не только по предположению, но и по своей статистике. Это усиливает ограничения GDA (где матрица произвольная) дополнительным условием диагональности.",
        "doc": "generativnyj-podhod-k-klassifikacii.md"
      },
      {
        "question": "В чём основное отличие бустинга от бэггинга при обучении ансамбля?",
        "answer": "В бустинге базовые алгоритмы обучаются последовательно: каждый следующий алгоритм учится на ошибках предыдущих. В бэггинге каждый алгоритм обучается независимо на случайной подвыборке данных.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Почему градиентный бустинг над решающими деревьями (GBDT) хорошо работает с табличными данными?",
        "answer": "Потому что деревья могут эффективно находить сложные, нелинейные зависимости между признаками, что характерно для неоднородных табличных данных, где признаки могут быть разного типа и природы.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Какую роль играет темп обучения (learning rate) в градиентном бустинге?",
        "answer": "Он регулирует, насколько сильно каждый новый алгоритм влияет на общее предсказание. Меньший темп обучения делает обучение более стабильным и снижает риск переобучения, но требует больше итераций.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Почему в градиентном бустинге следующий алгоритм обучается на антиградиенте функции потерь?",
        "answer": "Потому что движение в сторону антиградиента позволяет максимально эффективно уменьшать значение функции потерь. Это обобщает подход на любые дифференцируемые функции потерь.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "В чём идея метода MDI для оценки важности признаков?",
        "answer": "Он оценивает важность признака как среднюю долю объектов, для которых происходило ветвление по этому признаку во всех деревьях ансамбля. Чем чаще признак используется в верхних уровнях деревьев, тем он важнее.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Какие основные библиотеки реализуют градиентный бустинг?",
        "answer": "LightGBM, XGBoost и CatBoost. Они отличаются, в том числе, способом построения деревьев и внутренней оптимизацией.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Как строятся деревья в CatBoost и в чём преимущество такой схемы?",
        "answer": "В CatBoost все вершины одного уровня имеют одинаковый признак для ветвления. Это позволяет ускорить применение модели за счёт битовых операций и служит дополнительной регуляризацией.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Почему градиентный бустинг плохо подходит для задач с однородными данными, например, с изображениями?",
        "answer": "Потому что решающие деревья (и, соответственно, GBDT) не способны эффективно обрабатывать структурированные данные, такие как пиксели изображений. Для них лучше подходят нейронные сети.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Что такое «остатки» в контексте градиентного бустинга и когда их выгоднее использовать вместо антиградиента?",
        "answer": "Остатки — это разница между истинным значением и текущим предсказанием. Их выгоднее использовать, например, при MAE-функции потерь, так как это ускоряет сходимость и уменьшает зависимость от масштаба таргета.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Как градиентный бустинг связан с разложением функции в ряд Тейлора?",
        "answer": "Как и в разложении функции в ряд, где каждое следующее слагаемое уточняет приближение, в бустинге каждый новый алгоритм уточняет предсказание предыдущих, постепенно улучшая модель.",
        "doc": "gradientnyj-busting.md"
      },
      {
        "question": "Какие типы данных могут быть представлены в виде графов?",
        "answer": "Графы могут описывать социальные сети, дорожные или компьютерные сети, молекулярные структуры, графы знаний, цитирования и другие структуры, где важны связи между объектами.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Какие уровни задач можно решать с помощью графовых нейронных сетей?",
        "answer": "Можно решать задачи на уровне графа (graph-level), на уровне вершин (node-level) и на уровне рёбер (edge-level).",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "В чём основное отличие графовых нейронных сетей от сверточных?",
        "answer": "Графовые сети могут обрабатывать данные произвольного размера, в отличие от сверточных, где изображения должны быть фиксированного размера. Также графовые сети учитывают связи между элементами.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Какие два основных подхода используются при построении графовых свёрток?",
        "answer": "Пространственный (spatial) и спектральный (spectral). Пространственный работает через передачу сообщений между соседями, а спектральный — через анализ собственных векторов графового лапласиана.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Что такое message-passing в графовых нейронных сетях?",
        "answer": "Это процесс, при котором каждая вершина обновляет своё состояние на основе информации, собранной от соседних вершин и рёбер, соединяющих их.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Как работает архитектура GraphSAGE?",
        "answer": "Она собирает информацию от соседей вершины, агрегирует её (например, усреднением или максимумом), объединяет с текущим представлением вершины и применяет обучаемое преобразование.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Чем GAT отличается от GraphSAGE?",
        "answer": "GAT использует механизм внимания (attention), чтобы определить, какие соседи важнее при обновлении представления вершины, в то время как GraphSAGE применяет фиксированные операции агрегации.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Для каких графов подходит RGCN?",
        "answer": "RGCN разработан для графов с рёбрами разных типов. Каждый тип ребра влияет на обновление представлений с помощью отдельной обучаемой матрицы.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Что такое лапласиан графа и зачем он нужен в спектральных методах?",
        "answer": "Лапласиан — это матрица, описывающая структуру графа. Его собственные векторы помогают находить разрезы и кластеры в графе, что используется в спектральных методах.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Почему спектральные методы подходят для задач кластеризации графов?",
        "answer": "Потому что собственные векторы лапласиана описывают разбиения графа на компоненты с минимальным количеством рёбер между ними, что помогает выделять кластеры.",
        "doc": "grafovye-nejronnye-seti.md"
      },
      {
        "question": "Что означает метрика полноты (Coverage) в контексте рекомендательных систем?",
        "answer": "Это доля уникальных объектов, которые были рекомендованы пользователям, среди всех возможных объектов. Она показывает, насколько разнообразен ассортимент рекомендаций.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Как новизна (Novelty) рекомендаций связана с популярностью объектов?",
        "answer": "Чем менее популярный объект, тем выше его новизна. Это измеряется с помощью собственной информации: чем реже объект рекомендуется, тем более новым он считается для пользователя.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Как метрика разнообразия (Diversity) помогает улучшить пользовательский опыт?",
        "answer": "Разнообразие позволяет избежать однообразных рекомендаций, повышая интерес пользователя и снижая вероятность утомления от похожих объектов.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Что означает термин Serendipity в рекомендательных системах?",
        "answer": "Это способность рекомендовать объекты, которые не только релевантны пользователю, но и неожиданны, отличаются от того, с чем пользователь обычно взаимодействует.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Как можно оценить метрику Serendipity?",
        "answer": "Serendipity можно оценить, сравнив предсказания персонализированной модели с предсказаниями простой неперсонализированной модели: если персонализированная модель увереннее, это может свидетельствовать о серендипити.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Как алгоритм холодного старта влияет на полноту рекомендаций?",
        "answer": "Алгоритм холодного старта может ограничивать полноту, если новым пользователям или объектам выдаются только популярные элементы, что ухудшает шансы менее известных объектов быть рекомендованными.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Почему разнообразие важно для долгосрочного успеха рекомендательных систем?",
        "answer": "Потому что разнообразные рекомендации удерживают интерес пользователя, позволяя избежать эффекта насыщения и улучшая удовлетворенность сервисом.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Как можно использовать эмбеддинги объектов для оценки разнообразия?",
        "answer": "С помощью эмбеддингов можно измерить попарное сходство рекомендованных объектов, например, через косинусную близость, и усреднить его — это и будет метрикой разнообразия, например, ILS.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Какие факторы могут ограничивать рост полноты рекомендаций?",
        "answer": "К ним относятся: низкий трафик, бизнес-ограничения, слабая персонализация, отсутствие exploration-стратегий и особенности алгоритма холодного старта.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Почему метрики из этого раздела начинают использоваться не на начальном этапе?",
        "answer": "Потому что сначала нужно убедиться, что базовые бизнес-метрики (например, конверсия) находятся на удовлетворительном уровне, а затем уже можно оптимизировать более сложные свойства, такие как разнообразие и серендипити.",
        "doc": "horoshie-svojstva-rekomendatelnyh-sistem.md"
      },
      {
        "question": "Что означает термин implicit bias в контексте машинного обучения?",
        "answer": "Это явление, при котором алгоритм обучения выбирает среди всех возможных решений с нулевым эмпирическим риском только определённые, предпочитая одни решения другим.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Какое решение находит градиентный спуск с нулевой инициализацией в недоопределённой задаче линейной регрессии?",
        "answer": "Он находит решение с наименьшей евклидовой нормой среди всех возможных решений с нулевым эмпирическим риском.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Что такое псевдообратная матрица и как она используется в решении задачи линейной регрессии?",
        "answer": "Это обобщение обратной матрицы для случаев, когда матрица не квадратная или вырождена. В линейной регрессии она используется для нахождения решения с минимальной нормой.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Какой implicit bias наблюдается в градиентном спуске для линейных сетей?",
        "answer": "Градиентный спуск предпочитает решения малого ранга, обучаясь сначала на самых сильных модах (направлениях с наибольшей ковариацией).",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Что такое «сила моды» в контексте линейных сетей?",
        "answer": "Это мера ковариации между метками и входами в определённом направлении. Более сильные моды обучаются быстрее.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Как инициализация весов влияет на поведение градиентного спуска в линейных сетях?",
        "answer": "При инициализации весов вблизи нуля градиентный спуск сначала обучается на самых сильных модах, а более слабые остаются необученными в ближайшие моменты времени.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Какой эффект наблюдается при обучении разных мод с разной силой?",
        "answer": "Более сильные моды обучаются быстрее, а более слабые — медленнее. К моменту, когда сильная мода уже обучена, слабая может быть почти не обучена.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Какой тип решений предпочитает градиентный спуск в линейных сетях с фиксированным числом шагов?",
        "answer": "Он предпочитает решения малого ранга, которые соответствуют наилучшему приближению матрицы с заданным рангом.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Почему implicit bias важен при анализе обучения нейронных сетей?",
        "answer": "Потому что он показывает, какие из множества возможных решений с нулевым эмпирическим риском будет выбрано алгоритмом, и это может объяснять обобщающую способность модели.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "Что известно о implicit bias градиентного спуска в нелинейных сетях?",
        "answer": "На данный момент он остаётся почти не изученным, в отличие от линейных сетей, где поведение более понятно.",
        "doc": "implicit-bias.md"
      },
      {
        "question": "В чём основное отличие задачи рекомендаций от задачи поиска?",
        "answer": "В поиске пользователь формулирует запрос, а в рекомендациях запроса нет — система должна угадать интересы пользователя на основе его истории взаимодействий.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Что такое explicit и implicit фидбек в рекомендательных системах?",
        "answer": "Explicit — это чёткий сигнал, например, оценка или лайк. Implicit — это косвенный сигнал, например, клик или время просмотра, который не всегда точно отражает предпочтения.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Что такое ранжирующая модель в контексте рекомендаций?",
        "answer": "Это модель, которая оценивает релевантность объектов для пользователя и сортирует их по убыванию этой оценки, чтобы отобрать лучшие для показа.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Как работает подход коллаборативной фильтрации?",
        "answer": "Он основывается на схожести пользователей или объектов по истории взаимодействий: если два пользователя лайкали одни и те же объекты, им можно рекомендовать похожие.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "В чём преимущество content-based подхода перед коллаборативной фильтрацией?",
        "answer": "Он одинаково хорошо работает как с новыми, так и с известными объектами, потому что опирается на их содержание, а не на историю взаимодействий.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Что такое отбор кандидатов в рекомендательной системе?",
        "answer": "Это предварительный этап, на котором отбирается небольшое подмножество потенциально релевантных объектов, чтобы затем применить к ним более сложную и точную модель.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Какие методы используются при отборе кандидатов?",
        "answer": "Эвристики, коллаборативные методы (user2user, item2item), контентные эмбеддинги и индексация для поиска ближайших объектов.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Что такое реранжирование и зачем оно нужно?",
        "answer": "Это этап переупорядочивания рекомендаций после основного ранжирования, чтобы учесть бизнес-логику, например, разнообразие или свежесть контента.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Какую проблему решает feedback loop и как с ней борются?",
        "answer": "Feedback loop приводит к застреванию модели в локальном оптимуме, повторяя старые предпочтения. Против этого добавляют случайные объекты в выдачу и штрафуют за неожиданное поведение.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Какие недостатки есть у чисто коллаборативной фильтрации?",
        "answer": "Она не работает с новыми пользователями и объектами (проблема холодного старта) и может загонять пользователя в информационный пузырь, не предлагая ничего нового.",
        "doc": "intro-recsys.md"
      },
      {
        "question": "Что означает, что модель идеально калибрована?",
        "answer": "Это значит, что если модель предсказывает вероятность 0.7, то примерно в 70% случаев этот прогноз должен сбываться, то есть объект действительно принадлежит положительному классу.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Какие проблемы могут быть с калибровкой у сильных моделей, таких как нейросети?",
        "answer": "Такие модели могут быть переуверенными: они выдают вероятности, близкие к 0 или 1, даже когда не до конца уверены, из-за чего их предсказания не отражают истинную вероятность.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Почему логистическая регрессия не всегда предсказывает корректные вероятности?",
        "answer": "Потому что она может быть недообучена или переобучена, и если признаки плохо описывают данные, то модель будет выдавать неуверенные или, наоборот, ошибочно уверенные прогнозы.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Что такое гистограммная калибровка?",
        "answer": "Это метод, при котором диапазон предсказанных вероятностей разбивается на бины, и внутри каждого бина вероятность заменяется на среднюю долю положительных примеров в этом бине.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Как работает калибровка Платта?",
        "answer": "Это метод, при котором поверх предсказаний модели обучается сигмоида, чтобы откалибровать вероятности. Параметры сигмоиды подбираются на отложенной выборке.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Что такое изотоническая регрессия в контексте калибровки?",
        "answer": "Это метод, при котором предсказания отображаются в вероятности с помощью монотонной кусочно-постоянной функции, обучаемой с ограничением на порядок значений.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Как интерпретировать калибровочную кривую?",
        "answer": "Калибровочная кривая показывает, как соотносятся предсказанные вероятности и реальные доли положительных исходов. Идеально калиброванная модель будет на диагонали.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Что такое Brier score и для чего он используется?",
        "answer": "Это метрика, измеряющая среднюю квадратичную разницу между предсказанными вероятностями и истинными метками. Она оценивает качество калибровки.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Какой смысл у Expected Calibration Error?",
        "answer": "Это мера того, насколько средняя предсказанная вероятность в бинах отличается от доли положительных исходов в этих бинах. Чем меньше — тем лучше калибровка.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Почему калибровка важна в задачах, где вероятности используются для принятия решений?",
        "answer": "Потому что некорректные вероятности могут привести к неправильной оценке рисков и ошибкам в принятии решений, особенно в медицине, финансах и других критичных задачах.",
        "doc": "kak-ocenivat-veroyatnosti.md"
      },
      {
        "question": "Чем задача кластеризации отличается от задачи классификации?",
        "answer": "В задаче классификации известны классы и обучающая выборка с разметкой, а в задаче кластеризации разметки нет, и цель — выделить группы похожих объектов без заранее известных меток.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Какой подход используется в методе K-средних?",
        "answer": "Объекты распределяются по кластерам, и центры кластеров пересчитываются как среднее арифметическое точек, принадлежащих кластеру. Эти шаги повторяются до сходимости.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Как работает иерархическая агломеративная кластеризация?",
        "answer": "Алгоритм начинает с каждого объекта как отдельного кластера и последовательно объединяет ближайшие кластеры, пока не выполнится критерий останова.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Что такое дендрограмма и для чего она нужна?",
        "answer": "Это графическое представление иерархической кластеризации, где по вертикали отложены расстояния между кластерами при их объединении. Помогает визуализировать структуру кластеров.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Какие типы точек выделяют в DBSCAN?",
        "answer": "Основные (core), граничные (border) и шумовые (noise). Основные точки имеют в окрестности достаточно других точек, граничные — рядом с основными, но сами не являются основными, шумовые — не попадают ни в один кластер.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Какие метрики качества кластеризации не требуют разметки?",
        "answer": "Среднее внутрикластерное расстояние, среднее межкластерное расстояние и коэффициент силуэта.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Что означает гомогенность кластеров?",
        "answer": "Это мера того, насколько кластер состоит только из объектов одного класса. Она близка к 1, если каждый кластер содержит объекты одного и того же класса.",
        "doc": "klasterizaciya.md"
      },
      {
        "question": "Какие три основных типа рекомендательных систем выделяют в зависимости от используемой информации?",
        "answer": "Контентные, коллаборативные и гибридные.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Что лежит в основе контентных рекомендаций?",
        "answer": "Использование атрибутов объектов и пользователей для нахождения релевантных рекомендаций.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Какие признаки могут использоваться в контентных рекомендациях для объектов?",
        "answer": "Статистики объекта (например, количество лайков), признаки автора (например, жанр), а также неструктурированные данные (например, названия или эмбеддинги).",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Что такое факторизационные машины и зачем они нужны?",
        "answer": "Это модели, которые учитывают попарные взаимодействия между признаками, включая признаки пользователей и объектов, с использованием низкорангового приближения для уменьшения числа параметров.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Чем FFM (Field-aware Factorization Machines) отличаются от обычных факторизационных машин?",
        "answer": "FFM используют разные векторы для одного и того же признака в зависимости от того, с каким другим признаком он взаимодействует.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Как устроена модель DSSM?",
        "answer": "Это двухкомпонентная нейросеть, которая независимо генерирует эмбеддинги для двух сущностей (например, запроса и документа), а затем оценивает их схожесть.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Какая функция используется для оценки схожести в DSSM?",
        "answer": "Косинусная мера близости между эмбеддингами двух сущностей.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Что такое negative sampling и зачем он используется в обучении DSSM?",
        "answer": "Это метод, при котором на каждом шаге обучаются только положительные примеры и небольшое количество отрицательных, чтобы ускорить обучение.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Как трансформеры применяются в задачах рекомендаций?",
        "answer": "Трансформеры используются для обработки последовательностей взаимодействий пользователя, чтобы учитывать порядок и сложные зависимости в его поведении.",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Какие функции потерь могут использоваться в DSSM?",
        "answer": "Кросс-энтропия, попарные потери (pairwise loss) и триплет-потери (triplet loss).",
        "doc": "kontentnye-rekomendacii.md"
      },
      {
        "question": "Что такое краудсорсинг в контексте машинного обучения?",
        "answer": "Это способ сбора и разметки данных, при котором для выполнения задач привлекается большое количество людей, не обладающих специальными навыками, с целью создания обучающих датасетов для моделей машинного обучения.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Какова основная идея метода агрегации «мнение большинства»?",
        "answer": "Финальный ответ определяется как вариант, который выбрали большинство исполнителей. Он считается правильным, если большинство проголосовало за него.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Какие проблемы могут возникнуть с методом «мнение большинства»?",
        "answer": "Метод не учитывает различия в способностях исполнителей и разную сложность вопросов, что может привести к неточным результатам.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Что такое декомпозиция в краудсорсинге?",
        "answer": "Это принцип деления сложной задачи на множество небольших и простых микрозадач, каждая из которых может быть выполнена отдельным исполнителем по понятной инструкции.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Какие задачи в ML можно решать с помощью краудсорсинга?",
        "answer": "Сбор и разметка данных, например, транскрипция аудио, сегментация объектов на изображениях, проверка переводов, модерация контента.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Какие есть ограничения у краудсорсинга?",
        "answer": "Трудно использовать для задач, требующих поддержания контекста или высокой квалификации, например, перевод целой книги или выполнение специфичных профессиональных задач.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Что такое пайплайн в краудсорсинговом проекте?",
        "answer": "Это детальная схема проекта, состоящая из цепочки микрозадач и элементов, управляющих процессом, например, инструкциями и контролем качества.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Какие краудсорсинговые платформы используются для ML-проектов?",
        "answer": "Примеры — Amazon Mechanical Turk и Толока. Они позволяют заказчику создавать проекты, привлекать исполнителей и контролировать качество.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "В чём преимущество краудсорсинга при разметке данных?",
        "answer": "Он позволяет быстро и в больших масштабах собирать и размечать данные, что ускоряет разработку и обучение моделей машинного обучения.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Что такое «облако исполнителей»?",
        "answer": "Это большая группа людей, подключённых к краудсорсинговой платформе, готовых выполнять задания по разметке или сбору данных.",
        "doc": "kraudsorsing.md"
      },
      {
        "question": "Что такое метод hold-out в кросс-валидации?",
        "answer": "Это простое разделение данных на тренировочное и тестовое множества, при котором модель обучается на одном, а тестируется на другом.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Почему может быть важна стратификация при разбиении данных?",
        "answer": "Она сохраняет соотношение классов в тренировочном и тестовом множествах, что особенно важно при несбалансированных данных.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Как работает метод k-Fold кросс-валидации?",
        "answer": "Данные разбиваются на k частей, затем k раз обучается модель, каждый раз на k-1 частях, а тестируется на оставшейся части. Результаты усредняются.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "В чём особенность кросс-валидации для временных рядов?",
        "answer": "Данные нельзя перемешивать, и тестовая выборка всегда должна идти после тренировочной по времени, чтобы не использовать будущие данные.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Что такое метод leave-one-out?",
        "answer": "Это частный случай k-Fold, при котором каждый фолд состоит из одного примера. Модель обучается на всех примерах, кроме одного, и тестируется на нём.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Какие проблемы могут возникнуть, если не перемешивать данные перед разбиением?",
        "answer": "Модель может обучиться на определённом порядке данных, а не на их содержании, что приведёт к переобучению и плохой обобщающей способности.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Что такое «утечка данных» (data leakage) и как она может повлиять на модель?",
        "answer": "Это ситуация, когда модель косвенно или напрямую использует информацию из тестовой выборки, что приводит к завышенному качеству.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Какие признаки могут считаться «прокси» к таргету?",
        "answer": "Это признаки, которые тесно коррелируют с целевой переменной и могут её дублировать, что приводит к переобучению.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Что такое кривые обучения и зачем они нужны?",
        "answer": "Это графики, показывающие, как качество модели на тренировке и тесте меняется с ростом количества обучающих примеров. Они помогают понять, нужно ли добавлять данные.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Почему нельзя подбирать гиперпараметры на тестовом множестве?",
        "answer": "Это приведёт к утечке информации из тестовой выборки в модель, и итоговая оценка качества будет завышена.",
        "doc": "kross-validaciya.md"
      },
      {
        "question": "Почему для обучения нейросетей часто используют суррогатные функции потерь?",
        "answer": "Потому что настоящая функция риска может быть недифференцируемой, и суррогат позволяет использовать градиентную оптимизацию.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Почему функция потерь нейросети не всегда выпукла?",
        "answer": "Потому что даже при выпуклой функции потерь, сама нейросеть как функция своих весов может быть невыпуклой, как показывает простой пример с квадратичной функцией.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "В каких случаях все локальные минимумы нейросети будут глобальными?",
        "answer": "Это может быть доказано для линейных сетей и для достаточно широких нелинейных сетей, где ширина хотя бы одного слоя не меньше числа обучающих примеров.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Какие условия необходимы для доказательства глобальности всех локальных минимумов в широких сетях?",
        "answer": "Функция активации должна быть аналитичной, ограниченной и не равной нулю тождественно, ширина слоя — не меньше числа примеров, и все обучающие примеры должны быть различны.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Что такое лемма 1 в контексте доказательства теоремы о минимумах?",
        "answer": "Лемма утверждает, что при определённых условиях множество весов, при которых скрытые представления не имеют полного ранга, имеет меру ноль.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Что утверждает теорема 2 о глобальности минимума?",
        "answer": "Если сеть достигает локального минимума и выполнены определённые условия (например, на производную активации и ширину слоёв), то этот минимум будет глобальным.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Можно ли обобщить теорему о глобальности минимумов на более глубокие сети?",
        "answer": "Да, при условии, что один из слоёв достаточно широкий, и при выполнении дополнительных условий на функцию активации и структуру сети.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Что означает, что множество подуровня функции потерь связно?",
        "answer": "Это означает, что из любых двух точек в пространстве весов можно построить путь, на котором значение функции потерь не возрастает и достигает минимального значения.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Как ReLU влияет на доказательства о структуре ландшафта функции потерь?",
        "answer": "ReLU не является аналитичной функцией, и это мешает применению некоторых доказательств, основанных на аналитичности, например, в лемме 3.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Почему ширина сети важна для гарантий о минимумах?",
        "answer": "Достаточная ширина обеспечивает полный ранг матриц, что позволяет доказать, что все локальные минимумы являются глобальными.",
        "doc": "landshaft-funkcii-poter.md"
      },
      {
        "question": "Почему линейные модели могут быть неудобны для сложных задач?",
        "answer": "Потому что они представляют собой довольно узкий класс функций, и для сложных задач может потребоваться создание дополнительных признаков — процесс, называемый feature engineering.",
        "doc": "linear-models.md"
      },
      {
        "question": "Что такое метод наименьших квадратов?",
        "answer": "Это один из способов обучения линейной регрессии, при котором минимизируется сумма квадратов разностей между предсказаниями модели и истинными значениями целевой переменной.",
        "doc": "linear-models.md"
      },
      {
        "question": "Какой подход используется в задаче многоклассовой классификации с помощью логистической регрессии?",
        "answer": "Обучается по одной модели для каждого класса, которая отличает этот класс от всех остальных, а затем выбирается класс с наибольшим значением линейной функции.",
        "doc": "linear-models.md"
      },
      {
        "question": "Как регуляризация влияет на линейную модель?",
        "answer": "Она помогает избежать переобучения, добавляя штраф за слишком большие значения весов, что делает модель более устойчивой.",
        "doc": "linear-models.md"
      },
      {
        "question": "Почему в линейной модели может быть проблема с интерпретацией весов?",
        "answer": "Потому что веса могут меняться в зависимости от обучающей выборки, хотя при увеличении размера выборки они стремятся к устойчивым значениям.",
        "doc": "linear-models.md"
      },
      {
        "question": "Какое преимущество у SVM среди линейных моделей?",
        "answer": "Он обладает уникальным решением и доказуемо минимальной склонностью к переобучению среди популярных линейных классификаторов.",
        "doc": "linear-models.md"
      },
      {
        "question": "Что такое ядровый SVM?",
        "answer": "Это модификация SVM, позволяющая строить нелинейные разделяющие поверхности.",
        "doc": "linear-models.md"
      },
      {
        "question": "Какие задачи решаются с помощью линейных моделей?",
        "answer": "В основном задачи регрессии и классификации, включая многоклассовую.",
        "doc": "linear-models.md"
      },
      {
        "question": "Почему линейные модели полезно изучать?",
        "answer": "Потому что они являются основой для понимания более сложных моделей, включая нейронные сети, и многие принципы остаются теми же.",
        "doc": "linear-models.md"
      },
      {
        "question": "Как выбирается лучший класс в многоклассовой классификации с помощью логистической регрессии?",
        "answer": "Выбирается тот класс, значение линейной функции для которого оказалось наибольшим, то есть модель наиболее уверена в этом классе.",
        "doc": "linear-models.md"
      },
      {
        "question": "Что такое обучение с учителем?",
        "answer": "Это задача, в которой для каждого объекта из обучающей выборки известен правильный ответ (таргет). Цель — обучить модель предсказывать этот ответ для новых объектов.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Какие типы задач машинного обучения упоминаются в тексте?",
        "answer": "Обучение с учителем, обучение без учителя, а также генерация новых объектов и ранжирование.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Что такое переобучение?",
        "answer": "Это ситуация, когда модель слишком хорошо подстраивается под обучающие данные и теряет способность обобщать на новые, не виденные ранее примеры.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Какие бывают типы признаков?",
        "answer": "Численные, категориальные, бинарные и ординальные. Например, возраст — численный, пол — категориальный, пол — бинарный, уровень образования — ординальный.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Какие метрики качества бывают у задач машинного обучения?",
        "answer": "Бизнес-метрики, онлайн- и оффлайн-метрики, асессорские оценки. Например, точность, среднеквадратичная ошибка, полнота и точность.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Что такое функция потерь?",
        "answer": "Это функция, которую минимизируют при обучении модели, чтобы приблизить предсказания модели к правильным ответам.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Что такое data drift?",
        "answer": "Это изменение распределения данных со временем, например, из-за изменения схемы сбора или поведения пользователей.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Что такое модель в контексте машинного обучения?",
        "answer": "Это способ описания мира, отображающий признаки объекта в его предсказание. Например, линейная функция или дерево решений.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Какие бывают типы классификации?",
        "answer": "Бинарная (два класса), многоклассовая (несколько классов) и многоклассовая с пересекающимися классами (multilabel).",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Какие проблемы могут быть с данными?",
        "answer": "Пропуски, выбросы, ошибки разметки и data drift — всё это может повлиять на качество модели.",
        "doc": "mashinnoye-obucheniye.md"
      },
      {
        "question": "Что такое матричная факторизация?",
        "answer": "Это представление матрицы в виде произведения двух или более матриц, часто с определёнными свойствами, например, меньшей размерности или специальной структуры.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Какой смысл может быть у разложения матрицы в произведение с меньшей размерностью?",
        "answer": "Это позволяет упростить данные, выделив латентные признаки, и тем самым понизить размерность, что может улучшить обобщающую способность модели.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Какую задачу решает сингулярное разложение (SVD)?",
        "answer": "SVD помогает найти латентные признаки, приближает матрицу с наименьшей ошибкой в смысле нормы Фробениуса и может использоваться для понижения размерности.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Как можно использовать SVD в задачах рекомендательных систем?",
        "answer": "Можно разложить матрицу пользователь-товар в произведение, чтобы найти латентные признаки пользователей и товаров, а затем предсказывать рейтинги как скалярное произведение этих векторов.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Что такое ICA и в чём её отличие от SVD?",
        "answer": "ICA (анализ независимых компонент) ищет независимые компоненты, а не просто некоррелированные, как SVD. Она лучше подходит для разделения смешанных сигналов.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Почему может быть полезна регуляризация в моделях латентных факторов?",
        "answer": "Она помогает избежать переобучения, штрафуя за слишком большие значения в векторах латентных факторов.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Что такое NMF и когда его используют?",
        "answer": "NMF — неотрицательное матричное разложение. Его используют, когда важно, чтобы все компоненты разложения были неотрицательными, например, в тематическом моделировании.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Какой метод используется для оптимизации в ALS?",
        "answer": "В ALS чередуются шаги оптимизации одной матрицы при фиксированной другой, пока не будет достигнута сходимость.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Почему может быть важно центрировать данные перед применением SVD?",
        "answer": "Центрирование помогает избежать ситуации, когда первая компонента просто указывает на центр масс, а не на важные признаки.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "В чём суть модели латентных факторов?",
        "answer": "Модель предполагает, что наблюдаемые признаки или рейтинги можно объяснить скрытыми (латентными) факторами, взаимодействие между которыми описывается скалярным произведением.",
        "doc": "matrichnaya-faktorizaciya.md"
      },
      {
        "question": "Для чего нужно матричное дифференцирование в машинном обучении?",
        "answer": "Чтобы эффективно вычислять градиенты функций, зависящих от векторов и матриц, и использовать их в градиентных методах оптимизации.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Что такое дифференциал функции?",
        "answer": "Это линейное отображение, которое приближает изменение функции при малом изменении её аргумента.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Какой смысл у градиента скалярной функции?",
        "answer": "Градиент указывает направление наибольшего роста функции и используется в градиентных методах оптимизации.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Как выглядит дифференциал скалярной функции по матричному аргументу?",
        "answer": "Он выражается как скалярное произведение градиента функции и приращения матрицы.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Как связаны дифференциал и градиент в случае векторного аргумента?",
        "answer": "Дифференциал выражается через скалярное произведение градиента и приращения вектора.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Какой смысл у матрицы Якоби?",
        "answer": "Это матрица частных производных вектор-функции, описывающая линейное приближение этой функции в точке.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Что показывает второй дифференциал функции?",
        "answer": "Он описывает квадратичное приближение функции и помогает определить, является ли критическая точка минимумом или максимумом.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Как проверить, является ли точка минимумом с помощью второй производной?",
        "answer": "Нужно убедиться, что квадратичная форма второго дифференциала положительно определена.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Какие свойства производной используются при матричном дифференцировании?",
        "answer": "Линейность, правило произведения и правило дифференцирования сложной функции.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Почему важно уметь дифференцировать по матрицам?",
        "answer": "Потому что многие модели машинного обучения используют матрицы как параметры, и для их оптимизации нужны градиенты.",
        "doc": "matrichnoe-differencirovanie.md"
      },
      {
        "question": "Для чего используется метод обратного распространения ошибки?",
        "answer": "Чтобы эффективно вычислять градиенты функции потерь по параметрам нейронной сети, необходимые для обучения с помощью градиентного спуска.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Что происходит на этапе forward propagation?",
        "answer": "Вычисляются и запоминаются все промежуточные значения, которые будут использоваться при обратном проходе для вычисления градиентов.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Что такое backward propagation?",
        "answer": "Это этап, на котором градиенты функции потерь вычисляются по всем параметрам сети, начиная с конечного слоя и двигаясь к первому.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Почему нельзя просто вычислять производные по отдельности?",
        "answer": "Потому что это требует много памяти и вычислительных ресурсов, особенно при работе с матрицами и тензорами.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Какой принцип лежит в основе обратного распространения ошибки?",
        "answer": "Это применение цепного правила дифференцирования сложной функции, когда градиенты передаются от слоя к слою в обратном порядке.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Что делает autograd?",
        "answer": "Он автоматически вычисляет градиенты, позволяя разработчику сосредоточиться только на прямом проходе.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Как связаны forward и backward propagation?",
        "answer": "Forward propagation вычисляет значения, backward — градиенты, используя эти значения.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Какие слои участвуют в backward propagation?",
        "answer": "Все слои, начиная от выходного и до входного, каждый из которых передаёт градиент дальше.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Почему важно хранить промежуточные результаты при обучении?",
        "answer": "Они нужны для корректного вычисления градиентов в обратном проходе.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Что позволяет автоматическое дифференцирование (autograd) в обучении нейросетей?",
        "answer": "Оно упрощает процесс обучения, позволяя библиотеке автоматически вычислять градиенты, не требуя ручного кодирования backward pass.",
        "doc": "metod-obratnogo-rasprostraneniya-oshibki.md"
      },
      {
        "question": "Что такое критическая точка в контексте оптимизации?",
        "answer": "Это точка, в которой градиент функции равен нулю. В выпуклой оптимизации это может быть глобальный минимум, а в невыпуклой — локальный минимум, максимум или седловая точка.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Какие проблемы могут возникать у AdaGrad при обучении нейросетей?",
        "answer": "AdaGrad может слишком резко уменьшать скорость обучения из-за накопления квадратов градиентов, что особенно заметно при нестабильных архитектурах.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Что делает RMSprop по сравнению с AdaGrad?",
        "answer": "RMSprop использует экспоненциальное скользящее среднее квадратов градиентов, чтобы забывать старую информацию и не уменьшать скорость обучения слишком быстро.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Как работает momentum в методах оптимизации?",
        "answer": "Momentum накапливает информацию о предыдущих направлениях градиента, чтобы сгладить путь к минимуму и ускорить обучение.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Какую проблему решает Adam с помощью bias correction?",
        "answer": "Bias correction компенсирует смещение начальных оценок первого и второго моментов градиентов, чтобы улучшить стабильность в начале обучения.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Что такое AMSgrad и зачем он нужен?",
        "answer": "Это модификация Adam, которая исправляет проблемы с неубывающим адаптивным темпом обучения, обеспечивая лучшую сходимость.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Как learning rate scheduling помогает в обучении?",
        "answer": "Он позволяет изменять темп обучения в процессе обучения, например, уменьшая его для более точной сходимости к минимуму.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Почему Adam может хуже обобщаться, чем SGD?",
        "answer": "Исследования показывают, что адаптивные методы, как Adam, могут находить менее устойчивые решения, в то время как SGD лучше обобщается на тестовых данных.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Что такое Nesterov momentum?",
        "answer": "Это улучшенная версия momentum, где градиент вычисляется не в текущей точке, а в предполагаемом следующем положении.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "Какие методы подходят для разреженных данных?",
        "answer": "AdaGrad, RMSprop и Adam часто применяются для разреженных данных, но могут требовать корректировки, например, с помощью AMSgrad.",
        "doc": "metody-optimizacii-v-deep-learning.md"
      },
      {
        "question": "В чём основная идея метода Ньютона?",
        "answer": "Метод использует квадратичную аппроксимацию функции, чтобы определить направление спуска, и требует вычисления гессиана — матрицы вторых производных.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Какова скорость сходимости метода Ньютона?",
        "answer": "В окрестности точки оптимума метод Ньютона сходится квадратично, то есть ошибка уменьшается квадратично с каждой итерацией.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Почему метод Ньютона устойчив к плохой обусловленности задачи?",
        "answer": "Потому что он учитывает кривизну функции, в отличие от градиентного спуска, и инвариантен к линейным преобразованиям, что помогает быстрее сходиться.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Какая основная вычислительная проблема метода Ньютона?",
        "answer": "Требуется вычислять и обращать гессиан — матрицу вторых производных, что требует кубической сложности и много памяти.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Что такое квазиньютоновские методы?",
        "answer": "Это методы, которые приближают гессиан или его обратную матрицу, не вычисляя её напрямую, чтобы ускорить обучение и сэкономить память.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Как работает метод BFGS?",
        "answer": "Он приближает обратный гессиан с помощью специальной итеративной формулы, удовлетворяющей уравнению секущей, и не требует прямого вычисления гессиана.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "В чём преимущество L-BFGS по сравнению с BFGS?",
        "answer": "L-BFGS хранит только последние обновления, что позволяет ему использовать линейный объём памяти, в отличие от BFGS, который требует квадратичный объём.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Почему L-BFGS считается более практичным методом?",
        "answer": "Он требует линейного времени на итерацию и линейного объёма памяти, при этом сохраняет хорошую сходимость и требует только градиенты.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Как можно улучшить вычислительную эффективность метода Ньютона?",
        "answer": "Можно не обращать гессиан напрямую, а решать систему линейных уравнений итеративно, умножая гессиан на вектор с помощью автоматического дифференцирования.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Почему методы второго порядка не всегда применимы в стохастической оптимизации?",
        "answer": "Они требуют точного вычисления градиентов и, как правило, гессиана, что невозможно при использовании стохастических оценок градиента на больших датасетах.",
        "doc": "metody-vtorogo-poryadka.md"
      },
      {
        "question": "Что такое lazy learning в контексте метрических методов?",
        "answer": "Это подход, при котором модель не обучается на этапе тренировки, а просто запоминает обучающую выборку и делает предсказания на основе схожести новых объектов с уже известными.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Как работает метод k-ближайших соседей (KNN) при классификации?",
        "answer": "Алгоритм находит k ближайших объектов из обучающей выборки к новому объекту и присваивает ему класс, который наиболее часто встречается среди этих соседей.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Почему взвешенный KNN может быть лучше обычного?",
        "answer": "Потому что он учитывает расстояние до соседей: близкие объекты получают больший вес, чем дальние, что позволяет точнее предсказывать результат.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Какие метрики расстояния часто используются в KNN?",
        "answer": "Евклидово, манхэттенское, Минковского, косинусное и Жаккара — в зависимости от типа данных и задачи.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Что такое ядерная функция в контексте KNN?",
        "answer": "Это функция, которая задаёт вес для каждого соседа в зависимости от его расстояния до целевого объекта: чем ближе — тем больше вес.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Как KNN применяется в задаче регрессии?",
        "answer": "Предсказание делается как среднее (возможно, взвешенное) значение целевой переменной ближайших соседей.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Какие проблемы могут возникнуть при использовании KNN?",
        "answer": "Алгоритм чувствителен к масштабу признаков, требует много памяти и может быть медленным при больших объёмах данных.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Что такое k-d дерево и для чего оно используется?",
        "answer": "Это структура данных, которая делит пространство признаков на области, позволяя ускорить поиск ближайших соседей по сравнению с полным перебором.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "В чём суть приближённых методов поиска ближайших соседей?",
        "answer": "Они находят не самых близких соседей, а приближённо близких, что позволяет ускорить поиск за счёт небольшой потери точности.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Как работает метод LSH (Locality-Sensitive Hashing)?",
        "answer": "Он использует специальные хеш-функции, которые с большей вероятностью помещают близкие объекты в один и тот же бакет, ускоряя поиск соседей.",
        "doc": "metricheskiye-metody.md"
      },
      {
        "question": "Что такое confusion matrix (матрица ошибок) и какие значения она содержит?",
        "answer": "Confusion matrix — это таблица, показывающая, сколько объектов было правильно и неправильно классифицировано. Содержит true positive, false positive, true negative и false negative.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Какая разница между precision и recall?",
        "answer": "Precision — это доля правильно предсказанных положительных объектов среди всех предсказанных положительных. Recall — это доля правильно предсказанных положительных среди всех реальных положительных.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Что такое F1-мера и как она вычисляется?",
        "answer": "F1-мера — это среднее гармоническое между precision и recall. Она объединяет обе метрики в одну оценку качества.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Что означает AUC и для чего она используется?",
        "answer": "AUC — это площадь под ROC-кривой. Она показывает, насколько хорошо модель разделяет классы, и не зависит от порога бинаризации.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "В чём разница между offline и online метриками?",
        "answer": "Offline-метрики вычисляются до запуска модели в продакшн, например, на исторических данных. Online-метрики — на основе поведения пользователей в работающей системе.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Что такое accuracy и в каких случаях она может быть обманчивой?",
        "answer": "Accuracy — это доля правильно предсказанных меток. Она может быть обманчивой при сильном дисбалансе классов, когда модель просто предсказывает самый частый класс.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Какие бывают способы усреднения метрик в многоклассовой классификации?",
        "answer": "Микроусреднение — усреднение счётчиков из матрицы ошибок, макроусреднение — усреднение метрик по каждому классу отдельно.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Почему нельзя напрямую оптимизировать метрики precision и recall?",
        "answer": "Потому что они зависят от глобальных счётчиков TP, FP, FN и не могут быть рассчитаны на одном объекте, как функция потерь.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Что такое Average Precision и как она вычисляется?",
        "answer": "Average Precision — это среднее значение точности при разных уровнях полноты, вычисленное по кривой точность-полнота.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Как влияет выбор порога на precision и recall?",
        "answer": "При низком пороге recall увеличивается, а precision может уменьшаться. При высоком пороге precision может быть выше, а recall — ниже.",
        "doc": "metriki-klassifikacii-i-regressii.md"
      },
      {
        "question": "Что такое совместное распределение случайных величин?",
        "answer": "Это распределение, которое описывает вероятности для всех возможных комбинаций значений нескольких случайных величин, например, в виде таблицы или многомерного тензора.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Как получить маргинальное распределение из совместного?",
        "answer": "Маргинальное распределение получается путём суммирования или интегрирования совместного распределения по всем переменным, кроме одной.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Что означает, что случайные величины независимы?",
        "answer": "Это означает, что совместное распределение этих величин равно произведению их одномерных (маргинальных) распределений.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Как выглядит плотность многомерного нормального распределения?",
        "answer": "Плотность имеет гауссовский вид и полностью определяется своим средним значением и ковариационной матрицей.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Что такое ковариационная матрица случайного вектора?",
        "answer": "Это матрица, элементы которой — ковариации между парами компонент вектора. Диагональные элементы — это дисперсии компонент.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Как связаны независимость и маргинальные распределения?",
        "answer": "Если величины независимы, то их совместное распределение равно произведению маргинальных распределений.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Какое распределение используется для моделирования случайного выбора одного из нескольких классов?",
        "answer": "Для этого подходит категориальное (или multinoulli) распределение, которое является частным случаем мультиномиального.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Что такое формула свёртки для непрерывных случайных величин?",
        "answer": "Это формула, позволяющая найти плотность распределения суммы двух независимых случайных величин через интеграл от произведения их плотностей.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Где применяется распределение Дирихле?",
        "answer": "Оно используется, например, в задачах, где нужно моделировать распределение вероятностей на симплексе, как в тематическом моделировании или схеме Пойя.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Как изменится многомерное нормальное распределение при линейном преобразовании?",
        "answer": "Оно останется нормальным, но его среднее и ковариационная матрица изменятся в соответствии с матрицей преобразования.",
        "doc": "mnogomernye-raspredeleniya.md"
      },
      {
        "question": "Зачем нужны модели с латентными переменными?",
        "answer": "Чтобы моделировать скрытые зависимости в данных, например, кластеры, которые не видны напрямую, и лучше описывать сложные распределения, чем простые унимодальные.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Что такое мягкая кластеризация?",
        "answer": "Это подход, при котором объект может принадлежать сразу нескольким кластерам с разной степенью уверенности, а не жёстко относиться только к одному.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Что такое смесь распределений?",
        "answer": "Это вероятностное распределение, которое представляет собой сумму нескольких других распределений, взвешенных по вероятностям компонентов смеси.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Как устроена скрытая переменная в модели смеси распределений?",
        "answer": "Это переменная, которая указывает, из какой компоненты смеси был сгенерирован очередной объект, но она недоступна для наблюдения.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Что делает E-шаг EM-алгоритма?",
        "answer": "На E-шаге вычисляется апостериорное распределение скрытых переменных при текущих параметрах модели.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Что делает M-шаг EM-алгоритма?",
        "answer": "На M-шаге обновляются параметры модели, максимизируя ожидаемое значение логарифма правдоподобия по скрытым переменным.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Чем отличается жёсткий EM от обычного?",
        "answer": "В жёстком EM на E-шаге используется точечная оценка скрытой переменной, а не распределение, как в обычном.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Как работает вероятностный PCA?",
        "answer": "Это вероятностная модель, которая предполагает, что данные линейно зависят от скрытых факторов с добавлением шума, и параметры оцениваются с помощью EM-алгоритма.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Какие преимущества даёт вероятностный PCA по сравнению с обычным?",
        "answer": "Он позволяет работать с пропущенными значениями, даёт вероятностную интерпретацию и может быть частью более сложных моделей.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Какие задачи решает EM-алгоритм?",
        "answer": "EM-алгоритм используется для оценки параметров модели, когда есть скрытые переменные, максимизируя правдоподобие данных.",
        "doc": "modeli-s-latentnymi-peremennymi.md"
      },
      {
        "question": "Что такое модель скользящего среднего MA(q)?",
        "answer": "Это модель временного ряда, в которой текущее значение ряда выражается через фиксированное среднее и линейную комбинацию q предыдущих значений белого шума.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Что означает модель авторегрессии AR(p)?",
        "answer": "Это модель, в которой текущее значение ряда выражается через p предыдущих значений ряда и текущее значение белого шума.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Какая основная цель модели ARIMA?",
        "answer": "Модель ARIMA используется для прогнозирования нестационарных временных рядов, которые можно сделать стационарными с помощью дифференцирования.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Что такое оператор сдвига в контексте временных рядов?",
        "answer": "Это оператор, который сдвигает значения ряда на один шаг назад, например, текущее значение становится предыдущим.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Как определяется модель ARMA(p, q)?",
        "answer": "Это комбинация модели авторегрессии AR(p) и модели скользящего среднего MA(q), описывающая ряд как сумму компонентов от прошлых значений ряда и шума.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Что такое частичная автокорреляция (PACF)?",
        "answer": "Это корреляция между значениями ряда с учетом и исключением влияния промежуточных значений, используется для определения порядка модели AR.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Какой порядок модели AR можно определить с помощью PACF?",
        "answer": "Порядок модели AR(p) определяется как лаг, после которого значения частичной автокорреляции становятся близкими к нулю.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Что означает сезонная модель SARIMA?",
        "answer": "Это расширение модели ARIMA, которое учитывает сезонные зависимости в данных, например, повторяющиеся паттерны каждые s периодов.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Как используется модель ARIMAX?",
        "answer": "Это расширение ARIMA, которое включает внешние (экзогенные) переменные, влияющие на временной ряд.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Какие шаги включает процесс построения модели ARIMA?",
        "answer": "Анализ выбросов, стабилизация дисперсии, дифференцирование, выбор параметров p и q, подбор модели по критериям, построение прогноза и интервалов.",
        "doc": "modeli-vida-arima.md"
      },
      {
        "question": "Что такое глубинное обучение?",
        "answer": "Это совокупность нейросетевых подходов, в которых используются модели с несколькими слоями, позволяющие обучать представления данных автоматически, без ручного проектирования признаков.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Какие основные идеи лежат в основе глубинного обучения?",
        "answer": "Это стремление к end-to-end обучению всей системы целиком и автоматическое обучение представлений объектов на основе данных.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Почему нейросети стали популярны только к 2011 году?",
        "answer": "Техническая возможность и понимание, как обучать большие нейросети, появились только тогда, когда стало доступно достаточно вычислительных ресурсов и данных.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "В чём преимущество end-to-end подхода?",
        "answer": "Вместо обучения отдельных компонентов системы, обучается вся система целиком, что упрощает пайплайн и часто улучшает качество.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Что такое обучение представлений?",
        "answer": "Это автоматическое извлечение информативных признаков из данных, что позволяет не зависеть от ручной работы экспертов по отбору признаков.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Какие типы данных хорошо обрабатываются нейросетями?",
        "answer": "Нейросети особенно хороши для структурированных данных, таких как изображения, тексты, видео, графы и т.д.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Почему в глубинном обучении часто используется инженерный подход?",
        "answer": "Теория не всегда успевает за практикой, и многое в обучении нейросетей основывается на эмпирических наблюдениях и интуиции.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Какие задачи решает обучение представлений?",
        "answer": "Оно автоматизирует процесс построения признаков, позволяя извлекать их из данных без ручного вмешательства.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Что такое полносвязная нейросеть?",
        "answer": "Это простейшая архитектура нейросети, в которой каждый нейрон одного слоя соединён со всеми нейронами следующего слоя.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Какие навыки даёт изучение обратного распространения ошибки?",
        "answer": "Оно позволяет эффективно вычислять градиенты параметров нейросети, что необходимо для её обучения с помощью градиентного спуска.",
        "doc": "nejronnye-seti.md"
      },
      {
        "question": "Что такое облако точек и из чего оно состоит?",
        "answer": "Облако точек — это неструктурированный набор векторов, описывающих позиции точек в пространстве, и, опционально, дополнительные признаки, такие как цвет или интенсивность.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Как работает сенсор LiDAR?",
        "answer": "LiDAR посылает луч света, который отражается от поверхности и возвращается на детектор. По времени возвращения луча вычисляется расстояние до объекта.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Какие преимущества у RGB-D камер по сравнению с LiDAR?",
        "answer": "RGB-D камеры дают информацию о цвете каждой точки, что может быть полезно для задач сегментации и детекции.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Как работает архитектура PointNet?",
        "answer": "PointNet применяет одинаковую функцию (например, MLP) к каждой точке, а затем агрегирует признаки всех точек, чтобы получить глобальное представление об облаке.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Что добавляет архитектура PointNet++ по сравнению с PointNet?",
        "answer": "PointNet++ обрабатывает локальные окрестности точек, что позволяет лучше улавливать локальные паттерны, в отличие от PointNet, который работает с облаком целиком.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Какие проблемы есть у воксельных архитектур?",
        "answer": "Они требуют много вычислительных ресурсов из-за высокой размерности 3D-свёрток и могут не справляться с большими объёмами пространства.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Что такое цилиндрическая проекция в контексте облаков точек?",
        "answer": "Это способ представления облака точек как 2D-изображения, полученного проецированием точек на цилиндр, разворачиваемый в прямоугольник, чтобы применять 2D-свёртки.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Как архитектура PointPainting объединяет данные изображения и облака точек?",
        "answer": "Она сначала сегментирует изображение, а затем присваивает каждой точке облака метку класса из сегментированного изображения, добавляя её как признак.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Какие проблемы могут возникать при использовании цилиндрической проекции?",
        "answer": "Объекты, находящиеся далеко в 3D-пространстве, могут оказаться рядом в проекции, и размеры объектов будут зависеть от расстояния, что усложняет обучение.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Какие основные подходы к построению нейросетей для облаков точек выделяют в статье?",
        "answer": "Три основных подхода: обработка как графа (PointNet), вокселизация и проекция на 2D-поверхность.",
        "doc": "nejroseti-dlya-oblakov-tochek.md"
      },
      {
        "question": "Какие типы данных можно считать последовательностями?",
        "answer": "Это могут быть тексты, музыка, видео, временные ряды, траектории движения, последовательности аминокислот и другие данные, упорядоченные во времени или по другому признаку.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Что такое токенизация?",
        "answer": "Это процесс разбиения текста на отдельные токены — например, слова, символы или n-граммы — чтобы представить текст в виде последовательности.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Какие задачи не решаются напрямую с помощью простых архитектур, применяемых к векторам?",
        "answer": "Задачи, связанные с генерацией последовательностей произвольной длины, например, машинный перевод или генерация текста.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Почему свёрточные сети могут быть неоптимальны для работы с последовательностями?",
        "answer": "Потому что они не учитывают порядок элементов так естественно, как это делают рекуррентные сети, и могут упускать важные долгосрочные зависимости.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Что такое RNN и в чём её основная идея?",
        "answer": "RNN (рекуррентная нейросеть) — это архитектура, в которой информация передаётся от одного шага к другому, позволяя учитывать контекст из предыдущих элементов последовательности.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Какие проблемы могут возникать у простых RNN?",
        "answer": "Они могут испытывать трудности с обучением долгосрочным зависимостям из-за проблем с затуханием или взрывом градиента.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Какие модификации RNN помогают справляться с долгосрочными зависимостями?",
        "answer": "Такие как LSTM и GRU, которые вводят специальные механизмы для хранения и передачи информации на длинных интервалах.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Что такое seq2seq?",
        "answer": "Это архитектура, состоящая из двух частей — энкодера и декодера, которая позволяет преобразовывать одну последовательность в другую, например, в задачах перевода.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Какие задачи решает архитектура seq2seq?",
        "answer": "Она применяется для задач, где нужно генерировать выходную последовательность произвольной длины по входной, например, машинный перевод, суммаризация текста.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Какие современные подходы заменили или дополнили RNN в обработке последовательностей?",
        "answer": "Архитектуры, основанные на механизме внимания (attention mechanism), включая трансформеры, которые показали лучшие результаты в многих задачах.",
        "doc": "nejroseti-dlya-raboty-s-posledovatelnostyami.md"
      },
      {
        "question": "Что такое условная вероятность?",
        "answer": "Это вероятность наступления одного события при условии, что другое событие уже произошло.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Как вычисляется условная вероятность?",
        "answer": "Она вычисляется как отношение вероятности одновременного наступления двух событий к вероятности события-условия.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что означает, что события A и B независимы?",
        "answer": "Это значит, что наступление одного из них не влияет на вероятность наступления другого.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Как формулируется формула полной вероятности?",
        "answer": "Это сумма произведений вероятностей гипотез на условные вероятности события при этих гипотезах.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что говорит формула Байеса?",
        "answer": "Она позволяет вычислить вероятность гипотезы при наступлении события, зная априорные вероятности и правдоподобие.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что такое условное математическое ожидание?",
        "answer": "Это среднее значение случайной величины при условии, что другая случайная величина приняла определённое значение.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Какое свойство у условного математического ожидания называется law of total expectation?",
        "answer": "Это свойство гласит, что среднее значение условного ожидания равно обычному математическому ожиданию.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что такое условная независимость?",
        "answer": "Это когда два события становятся независимыми при условии наступления третьего события.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Как определяется условное распределение в непрерывном случае?",
        "answer": "Оно задаётся условной плотностью, равной совместной плотности, делённой на маргинальную плотность условия.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что такое функция регрессии?",
        "answer": "Это условное математическое ожидание зависимой переменной при фиксированном значении независимой переменной.",
        "doc": "nezavisimost-i-uslovnye-raspredeleniya-veroyatnostej.md"
      },
      {
        "question": "Что такое нормализующий поток?",
        "answer": "Это обратимое и дифференцируемое преобразование, которое отображает простое распределение в более сложное, позволяя точно вычислять плотность вероятности и генерировать данные.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Какие задачи решают нормализующие потоки?",
        "answer": "Они позволяют генерировать новые данные, оценивать плотность вероятности и обучать скрытые представления объектов.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Почему нормализующие потоки отличаются от VAE и GAN?",
        "answer": "Потому что они позволяют точно вычислять плотность вероятности, в отличие от VAE, и обучаются стабильнее, чем GAN.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Что такое Planar Flow?",
        "answer": "Это простая архитектура нормализующего потока, в которой преобразование задаётся специальной функцией с низкоранговой матрицей, позволяющей быстро вычислять якобиан.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Какие типы связывания (coupling) используются в нормализующих потоках?",
        "answer": "Аддитивное (additive) и аффинное (affine) связывание, при которых часть переменных остаётся неизменной, а другая часть преобразуется с учётом первой.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Как работает WaveGlow?",
        "answer": "Это модель на основе нормализующих потоков, которая генерирует аудио по спектрограмме, используя архитектуру, похожую на WaveNet.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Что такое условная генерация в контексте нормализующих потоков?",
        "answer": "Это генерация данных при наличии дополнительной информации, например, текста или спектрограммы, которая используется как обусловливающий признак.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Какие проблемы есть у нормализующих потоков при детекции аномалий?",
        "answer": "Они могут оценивать плотность не по семантике, а по локальным графическим признакам, из-за чего аномальные объекты могут иметь высокую плотность.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Какие архитектурные особенности использует Glow?",
        "answer": "Glow использует обратимые свёртки 1x1 для перемешивания каналов и специальный слой нормализации actnorm для улучшения сходимости.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "В чём преимущество IAF (Inverse Autoregressive Flows) при генерации данных?",
        "answer": "IAF позволяет быстро генерировать данные, обучаясь на латентных переменных, зависимость которых строится авторегрессивно, а не на самих данных.",
        "doc": "normalizuyushie-potoki.md"
      },
      {
        "question": "Что такое эмпирический риск?",
        "answer": "Это средняя ошибка модели на обучающей выборке, которую мы можем вычислить и минимизировать, в отличие от истинного риска, который неизвестен.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Почему оценка обобщающей способности по эмпирическому риску может быть бессмысленной для очень сложных моделей?",
        "answer": "Потому что сложные модели, такие как глубокие нейросети, могут запомнить обучающую выборку и показывать нулевой эмпирический риск, но при этом иметь плохое качество на тесте.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Что такое равномерная оценка обобщающей способности?",
        "answer": "Это оценка, которая ограничивает разницу между истинным и эмпирическим рисками для всех моделей в заданном классе, независимо от того, какая из них была обучена.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Для чего используется неравенство Хёффдинга в контексте оценки обобщающей способности?",
        "answer": "Для оценки вероятности того, что разница между истинным и эмпирическим риском для фиксированной модели будет превышать заданное значение.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Что такое сложность Радемахера?",
        "answer": "Это мера способности класса функций коррелировать с чисто случайным шумом. Она используется для оценки обобщающей способности.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Как связана VC-размерность с функцией роста?",
        "answer": "VC-размерность — это максимальный размер множества, которое класс функций может разделить всеми возможными способами. Функция роста ограничена через VC-размерность с помощью леммы Сауэра-Шелаха.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Что означает «симметризация» в выводе оценок через сложность Радемахера?",
        "answer": "Это техника, при которой исходное выражение заменяется на эквивалентное, но зависящее от двух независимых выборок, что позволяет ввести шум Радемахера.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Какие проблемы могут возникать с равномерными оценками при использовании очень сложных моделей?",
        "answer": "Равномерные оценки могут становиться слишком грубыми или бессмысленными, если класс моделей слишком велик, например, если он включает все возможные реализации архитектуры.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Почему в нейросетях оценки через сложность Радемахера часто оказываются бесполезными?",
        "answer": "Потому что VC-размерность и сложность Радемахера для нейросетей могут быть экспоненциально большими или масштабироваться хуже, чем размер обучающей выборки.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Что такое «фундаментальная проблема равномерных оценок»?",
        "answer": "Это ситуация, когда равномерная оценка разности истинного и эмпирического рисков не объясняет хорошую обобщающую способность, потому что существуют модели с малым истинным, но большим эмпирическим риском.",
        "doc": "obobshayushaya-sposobnost-klassicheskaya-teoriya.md"
      },
      {
        "question": "Что такое обобщённая линейная модель (GLM)?",
        "answer": "Это класс вероятностных моделей, в которых предполагается, что зависимая переменная подчиняется некоторому распределению из экспоненциального семейства, а её математическое ожидание связано с линейной комбинацией признаков через функцию связи.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какие компоненты входят в определение GLM?",
        "answer": "Семейство распределений, функция связи и линейная модель для параметра распределения.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Что такое функция связи в GLM?",
        "answer": "Это функция, которая связывает линейную комбинацию признаков с математическим ожиданием зависимой переменной.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какая функция связи используется в линейной регрессии?",
        "answer": "Тождественная функция, то есть предсказание напрямую равно линейной комбинации признаков.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какая функция связи используется в логистической регрессии?",
        "answer": "Логит-функция, которая связывает линейную комбинацию признаков с логарифмом отношения вероятностей.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Почему экспоненциальный класс распределений важен для GLM?",
        "answer": "Потому что для распределений из этого класса удобно вычислять математическое ожидание и находить каноническую функцию связи.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Что такое каноническая функция связи?",
        "answer": "Это функция связи, при которой линейная комбинация признаков равна натуральному параметру распределения из экспоненциального семейства.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какую задачу решает пуассоновская регрессия?",
        "answer": "Она применяется для моделирования счётных данных, например, числа событий за фиксированный промежуток времени.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Почему линейная регрессия может быть неудобна для предсказания счётных данных?",
        "answer": "Потому что она может предсказывать отрицательные значения, а счётные данные всегда неотрицательны и целочисленны.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какое распределение часто используется в GLM для моделирования бинарных исходов?",
        "answer": "Распределение Бернулли.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Какой параметр определяет форму распределения в экспоненциальном семействе?",
        "answer": "Натуральный параметр, который часто связан с математическим ожиданием через функцию связи.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Как можно интерпретировать функцию cloglog?",
        "answer": "Это функция связи, которая может быть полезна в задачах с редкими положительными событиями, так как она асимметрична и лучше моделирует хвосты распределения.",
        "doc": "obobshyonnye-linejnye-modeli.md"
      },
      {
        "question": "Что такое обучение представлений?",
        "answer": "Это процесс настройки алгоритма для построения признакового описания объекта на основе его сырых данных, например, изображения или текста.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Почему важно обучать хорошие представления?",
        "answer": "Хорошие представления позволяют лучше решать задачи машинного обучения, так как они кодируют полезную информацию о данных в удобной для модели форме.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Какие типы обучения представлений выделяют в документе?",
        "answer": "Supervised, self-supervised и метрическое обучение (metric learning).",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Что такое fine-tuning (дообучение)?",
        "answer": "Это процесс дообучения модели, предварительно обученной на одной задаче, на другой, более конкретной задаче, часто с меньшим набором данных.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Какие задачи решаются с помощью self-supervised обучения?",
        "answer": "Задачи, где для обучения используются синтетические метки, извлечённые из самих данных, например, предсказание следующего слова или восстановление замаскированного патча.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Что такое triplet loss и зачем он нужен?",
        "answer": "Это функция потерь, используемая для обучения метрических эмбеддингов, которая притягивает похожие объекты и отталкивает непохожие.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Как работает метод SimCLR?",
        "answer": "Он генерирует разные аугментации одного и того же изображения, обучает модель притягивать их и отталкивать аугментации других изображений.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Почему важно перемешивать данные при разбиении на train и test?",
        "answer": "Чтобы распределения в тренировочной и тестовой выборках были похожи и модель не обучалась на систематических паттернах, связанных с порядком данных.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Как Vision Transformer (ViT) адаптирует подход BERT для изображений?",
        "answer": "ViT разбивает изображение на патчи, маскирует некоторые из них и обучается предсказывать их содержимое, аналогично маскировке токенов в BERT.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Какие преимущества даёт использование контекста при обучении представлений?",
        "answer": "Контекст позволяет модели лучше понимать структуру данных и выучивать более семантически богатые и обобщаемые представления.",
        "doc": "obuchenie-predstavleniyam.md"
      },
      {
        "question": "Что такое обучение с подкреплением?",
        "answer": "Это подход к машинному обучению, при котором алгоритм (агент) взаимодействует со средой, получает за свои действия скалярную награду и учится выбирать действия, максимизирующие суммарную награду.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое MDP?",
        "answer": "Марковский процесс принятия решений — это формализм, описывающий задачу обучения с подкреплением, состоящий из пространства состояний, действий, вероятностей переходов и функции награды.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Какова цель агента в задаче обучения с подкреплением?",
        "answer": "Цель агента — выбрать стратегию, которая максимизирует ожидаемую суммарную дисконтированную награду.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое стратегия в RL?",
        "answer": "Это правило, определяющее, какое действие должен выбрать агент в каждом состоянии.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое Q-функция?",
        "answer": "Это функция, которая оценивает суммарную ожидаемую награду при выполнении определённого действия в определённом состоянии и последующем следовании определённой стратегии.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "В чём суть дилеммы exploration vs exploitation?",
        "answer": "Это компромисс между исследованием новых действий, чтобы узнать о них больше, и использованием уже известной информации для получения текущей награды.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое off-policy обучение?",
        "answer": "Это когда агент обучается на данных, собранных по другой стратегии, отличной от текущей стратегии, по которой он принимает решения.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое replay buffer?",
        "answer": "Это хранилище, в которое записываются переходы (состояние, действие, награда, следующее состояние), и из которого затем сэмплируются батчи для обучения, чтобы разрушить корреляцию между примерами.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Какие есть основные типы алгоритмов в обучении с подкреплением?",
        "answer": "Основные типы — это табличные методы (например, Q-обучение), глубинное обучение с подкреплением (Deep RL), политические градиенты и Actor-Critic методы.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Как можно использовать обучение с подкреплением в реальных задачах?",
        "answer": "Например, для управления роботами, игры ИИ с человеком, настройки систем, адаптивного управления трафиком или персонализированных рекомендаций.",
        "doc": "obuchenie-s-podkrepleniem.md"
      },
      {
        "question": "Что такое онлайн-обучение?",
        "answer": "Это подход, при котором модель обновляется по мере поступления новых данных, например, по одному объекту или небольшому батчу, а не обучается целиком на всей выборке сразу.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "В чём разница между online и batch обучением?",
        "answer": "В batch обучении модель обучается на всей выборке целиком, а в online — по мере поступления новых данных, что позволяет адаптироваться к изменениям в данных.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что такое regret в контексте онлайн-обучения?",
        "answer": "Это разница между суммарной ошибкой онлайн-алгоритма и суммарной ошибкой наилучшего фиксированного алгоритма из некоторого класса, выбранного задним числом.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Какой смысл у метода Follow the Leader (FTL)?",
        "answer": "На каждом шаге FTL выбирает модель, которая наилучшим образом объясняет все предыдущие данные, минимизируя эмпирический риск на прошлых примерах.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Почему FTL может работать нестабильно?",
        "answer": "Потому что он может сильно колебаться между решениями, особенно если функции потерь имеют противоречивую структуру, что приводит к линейному росту regret.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что такое FTRL и зачем в него добавляют регуляризатор?",
        "answer": "FTRL — это метод онлайн-обучения, который минимизирует сумму потерь и регуляризатора. Регуляризатор стабилизирует процесс и помогает избежать осцилляций.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что такое линеаризация в контексте FTRL?",
        "answer": "Это замена выпуклой функции потерь её линейной аппроксимацией, что упрощает задачу оптимизации и делает её вычислительно эффективной.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Какие преимущества даёт использование адаптивных регуляризаторов в FTRL?",
        "answer": "Они позволяют алгоритму адаптироваться к данным, улучшая сходимость, и лежат в основе таких методов, как AdaGrad, RMSprop и Adam.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Как связаны онлайн-обучение и стохастическая оптимизация?",
        "answer": "Алгоритмы онлайн-обучения можно применять для решения задач стохастической оптимизации, например, с помощью преобразования online-to-batch, где regret используется для оценки сходимости.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что такое субградиент и когда он используется?",
        "answer": "Это обобщение градиента для негладких выпуклых функций. Используется в методах оптимизации, когда функция не дифференцируема в некоторых точках.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что означает 'сублинейный regret'?",
        "answer": "Это значит, что regret растёт медленнее, чем линейно, что говорит о том, что алгоритм со временем становится почти таким же хорошим, как и лучший фиксированный алгоритм.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Какие задачи подходят для онлайн-обучения?",
        "answer": "Задачи, где данные поступают последовательно, распределение данных может меняться со временем, или где важно быстро адаптироваться к новой информации.",
        "doc": "onlajn-obuchenie-i-stohasticheskaya-optimizaciya.md"
      },
      {
        "question": "Что такое hold-out метод валидации?",
        "answer": "Это простой способ оценки модели, при котором данные делятся на обучающую и тестовую выборки, и модель обучается на одной части, а качество измеряется на другой.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Какова основная идея стохастического градиентного спуска (SGD)?",
        "answer": "Вместо вычисления градиента по всему датасету, SGD использует оценку градиента по небольшому подмножеству данных (батчу), что ускоряет обучение.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Зачем нужна стратификация при разбиении данных?",
        "answer": "Чтобы сохранить пропорции классов в обучающей и тестовой выборках, особенно если классы несбалансированы.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Что такое k-Fold кросс-валидация?",
        "answer": "Это метод, при котором данные разбиваются на k частей, и модель обучается k раз, каждый раз используя k-1 часть для обучения и оставшуюся для валидации.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Как работает метод momentum в оптимизации?",
        "answer": "Он добавляет к шагу градиентного спуска накопленный импульс предыдущих шагов, что помогает быстрее проходить плато и избегать локальных минимумов.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "В чём идея Nesterov momentum?",
        "answer": "Это улучшенная версия momentum, где градиент вычисляется не в текущей точке, а в предполагаемом следующем положении, что позволяет лучше корректировать направление.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Как работает адаптивный метод Adagrad?",
        "answer": "Он уменьшает размер шага для признаков с большими градиентами и увеличивает для признаков с малыми, суммируя квадраты градиентов.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Чем RMSProp отличается от Adagrad?",
        "answer": "RMSProp использует скользящее среднее квадратов градиентов, а не сумму, что позволяет избежать слишком быстрого уменьшения шага.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Что такое Adam и какие идеи он объединяет?",
        "answer": "Это метод оптимизации, который объединяет идеи momentum и RMSProp, используя адаптивный размер шага и накопленный импульс.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Какую проблему решает метод SWA (Stochastic Weight Averaging)?",
        "answer": "SWA усредняет веса модели по ходу обучения, что помогает улучшить обобщающую способность и находить более устойчивые решения.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Какие проблемы могут возникнуть при использовании слишком большого размера батча?",
        "answer": "Может ухудшиться обобщающая способность модели, так как оптимизатор может застревать в узких локальных минимумах, и увеличивается риск переобучения.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Что такое LARS и когда он полезен?",
        "answer": "Это оптимизатор, который масштабирует размер шага для каждого слоя отдельно, что позволяет эффективно обучаться с большими батчами.",
        "doc": "optimizaciya-v-ml.md"
      },
      {
        "question": "Что такое PAC-байесовские оценки?",
        "answer": "Это методы оценки разницы между истинным и эмпирическим риском, которые используют априорное и апостериорное распределения на множестве моделей, чтобы учесть предпочтения алгоритма обучения.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Какую роль играет априорное распределение в PAC-байесовских оценках?",
        "answer": "Оно отражает наши априорные предположения о том, какие модели алгоритм обучения предпочитает, и помогает улучшить оценку, если апостериорное распределение близко к априорному.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Что такое апостериорное распределение в контексте PAC-байеса?",
        "answer": "Это распределение на множестве моделей, которое зависит от обучающих данных и описывает, какие модели чаще выбираются алгоритмом обучения после наблюдения данных.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Почему стохастичность алгоритма обучения важна для PAC-байесовских оценок?",
        "answer": "Потому что оценки основаны на KL-дивергенции между априорным и апостериорным распределениями, которая теряет смысл, если апостериорное распределение вырождается в одну точку (как у детерминированного алгоритма).",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Как PAC-байесовские оценки связаны с гипотезой о плоских минимумах?",
        "answer": "Если минимум плоский, то можно добавить шум к весам модели без значительного роста ошибки на тренировке, что уменьшает KL-дивергенцию и даёт лучшую оценку обобщающей способности.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Как можно применить PAC-байесовские оценки к детерминированному алгоритму?",
        "answer": "Можно добавить шум к финальной модели, полученной детерминированным алгоритмом, чтобы получить апостериорное распределение, или использовать дискретное кодирование модели.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Что означает «непустая» (non-vacuous) оценка обобщающей способности?",
        "answer": "Это оценка, которая даёт осмысленное численное значение, например, вероятность ошибки, меньшую 1, в отличие от тривиальных или заведомо завышенных оценок.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Какую информацию можно использовать при построении априорного распределения?",
        "answer": "Можно использовать знания о том, какие модели алгоритм обучения, как правило, предпочитает, или какие модели являются более простыми или сжатыми.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Как сжатие модели связано с PAC-байесовскими оценками?",
        "answer": "Сжатые модели, как предполагается, соответствуют более короткому коду, что уменьшает KL-дивергенцию и может привести к лучшей оценке риска.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Какие преимущества даёт использование PAC-байесовских оценок?",
        "answer": "Они позволяют учитывать структуру алгоритма обучения и могут давать нетривиальные оценки обобщающей способности даже для сложных моделей, таких как нейронные сети.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Какова цель введения априорного и апостериорного распределений?",
        "answer": "Цель — учесть, какие модели алгоритм обучения предпочитает, и тем самым получить более точную и информативную оценку обобщающей способности.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Почему оценка разности рисков важна в задачах машинного обучения?",
        "answer": "Потому что она помогает понять, насколько модель будет хорошо работать на новых данных, и позволяет оценить степень переобучения.",
        "doc": "pac-bajesovskie-ocenki-riska.md"
      },
      {
        "question": "Что такое i.i.d. выборка?",
        "answer": "Это выборка, состоящая из независимых одинаково распределённых случайных величин.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что утверждает закон больших чисел?",
        "answer": "Он говорит, что выборочное среднее сходится по вероятности к истинному среднему при увеличении размера выборки.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "В чём разница между сходимостью по вероятности и почти наверное?",
        "answer": "Сходимость по вероятности означает, что вероятность отклонения стремится к нулю, а почти наверное — что отклонение стремится к нулю с вероятностью 1.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что такое несмещённая оценка параметра?",
        "answer": "Это оценка, математическое ожидание которой равно истинному значению оцениваемого параметра.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что означает, что оценка является состоятельной?",
        "answer": "Это значит, что с ростом размера выборки оценка сходится по вероятности к истинному значению параметра.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Какова связь между MSE, смещением и дисперсией оценки?",
        "answer": "MSE оценки равен сумме квадрата смещения и дисперсии оценки.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что такое метод моментов?",
        "answer": "Это способ оценки параметров, при котором выборочные моменты приравниваются к теоретическим.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Какова идея метода максимального правдоподобия?",
        "answer": "Нужно выбрать параметры так, чтобы вероятность получить имеющуюся выборку была максимальной.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что такое асимптотическая нормальность оценки?",
        "answer": "Это свойство, при котором распределение оценки приближается к нормальному с увеличением размера выборки.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Почему выборочная дисперсия с делением на N-1 считается несмещённой?",
        "answer": "Потому что деление на N-1 компенсирует смещение, возникающее из-за использования выборочного среднего вместо истинного.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Какие свойства имеет оценка максимального правдоподобия при определённых условиях?",
        "answer": "Она асимптотически нормальна, состоятельна, инвариантна и асимптотически оптимальна по дисперсии.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Что такое эффективность оценки?",
        "answer": "Это свойство оценки иметь наименьшую возможную дисперсию среди всех несмещённых оценок.",
        "doc": "parametricheskiye-ocenki.md"
      },
      {
        "question": "Какие две основные категории входят в рабочее окружение для ML-специалиста?",
        "answer": "Железо и вычислительные ресурсы для обучения моделей, а также программы и библиотеки для работы с данными.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Какие популярные фреймворки для глубинного обучения упоминаются в тексте?",
        "answer": "TensorFlow и PyTorch.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Чем IaaS отличается от SaaS в контексте машинного обучения?",
        "answer": "IaaS — это мощный удалённый компьютер, который нужно настраивать самостоятельно, а SaaS — это готовые платформы с уже настроенным окружением для ML.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Какие библиотеки упоминаются как основные для работы с данными и ML в Python?",
        "answer": "Scikit-learn, Pandas, Matplotlib и Seaborn.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Почему не всегда возможно обучать модели на домашнем компьютере?",
        "answer": "Потому что для обучения сложных моделей требуется много вычислительных ресурсов, особенно видеопамяти GPU.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Какие популярные SaaS-платформы для машинного обучения упоминаются в тексте?",
        "answer": "Google Colab, Kaggle Notebooks, AWS SageMaker, Azure ML Studio и Yandex DataSphere.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Как получить доступ к Yandex DataSphere через тестовый грант?",
        "answer": "Можно воспользоваться тестовым грантом, доступ к которому можно получить по ссылке, указанной в тексте.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Что такое IDE и зачем она нужна?",
        "answer": "Это текстовый редактор для кода, например, Jupyter или PyCharm, который помогает писать и запускать программы.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Какова основная цель лабораторной работы, описанной в параграфе?",
        "answer": "Обучить генеративную трансформерную модель с помощью библиотеки transformers.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Почему PyTorch часто выбирают для академических исследований?",
        "answer": "Потому что он более гибкий и подходит для экспериментов.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Что означает «дистилляция знаний» в контексте нейросетей?",
        "answer": "Это способ обучения, при котором знания передаются от модели-учителя к модели-ученику, чтобы упростить и ускорить последнюю.",
        "doc": "distillyaciya-znanij.md"
      },
      {
        "question": "Какие библиотеки помогают визуализировать данные в Python?",
        "answer": "Matplotlib и Seaborn.",
        "doc": "pervye-shagi.md"
      },
      {
        "question": "Что такое нейронная сеть с точки зрения математики?",
        "answer": "Это сложная дифференцируемая функция, которая преобразует входные признаки в выходные значения, и все её параметры могут обучаться одновременно.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какие основные типы слоёв упоминаются в тексте?",
        "answer": "Линейный слой (dense) и функция активации (например, ReLU или сигмоида).",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Что такое forward pass в нейросети?",
        "answer": "Это процесс вычисления выхода сети по входным данным, когда информация проходит от входа к выходу через все слои.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Что такое backward pass?",
        "answer": "Это процесс, при котором градиенты функции потерь распространяются от выхода к входу, чтобы обновить параметры модели.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Зачем нужны функции активации?",
        "answer": "Они добавляют нелинейность в сеть, позволяя ей моделировать сложные зависимости, иначе последовательность линейных слоёв эквивалентна одному линейному слою.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какие задачи можно решать с помощью нейросетей по тексту?",
        "answer": "Бинарная и многоклассовая классификация, регрессия, а также задачи с разными типами таргетов одновременно.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какой функцией активации часто пользуются для бинарной классификации на выходе?",
        "answer": "Сигмоидой, чтобы получить значение от 0 до 1, которое можно интерпретировать как вероятность.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какая функция потерь обычно используется для многоклассовой классификации?",
        "answer": "Кросс-энтропия, она сравнивает предсказанное распределение вероятностей с истинным.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Что такое полносвязная нейросеть?",
        "answer": "Это сеть, состоящая только из линейных слоёв и функций активации, где каждый нейрон соединён со всеми нейронами следующего слоя.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какие популярные функции активации упоминаются?",
        "answer": "ReLU, Leaky ReLU, PReLU, ELU, сигмоида и гиперболический тангенс (tanh).",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "В чём преимущество ReLU по сравнению с сигмоидой?",
        "answer": "ReLU проще в вычислениях и меньше подвержена проблеме затухания градиента.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Что говорит теорема Цыбенко о нейросетях?",
        "answer": "Она утверждает, что двухслойная нейросеть с сигмоидной активацией может приблизить любую непрерывную функцию с заданной точностью.",
        "doc": "pervoe-znakomstvo-s-polnosvyaznymi-nejrosetyami.md"
      },
      {
        "question": "Какой метод подбора гиперпараметров перебирает все возможные комбинации значений?",
        "answer": "Grid Search (или полный перебор).",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой метод подбора гиперпараметров выбирает случайные комбинации значений из заданных распределений?",
        "answer": "Random Search (или случайный поиск).",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "В чем основное преимущество Random Search перед Grid Search?",
        "answer": "Он более эффективно исследует пространство гиперпараметров и может найти хорошие комбинации быстрее, особенно когда важны не все параметры.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой метод подбора гиперпараметров строит вероятностную модель зависимости метрики от гиперпараметров?",
        "answer": "Байесовская оптимизация.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Что такое гиперпараметры модели?",
        "answer": "Это параметры, значения которых не определяются автоматически в процессе обучения модели, а задаются вручную до начала обучения (например, скорость обучения, количество деревьев в ансамбле).",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой недостаток имеет метод Grid Search?",
        "answer": "Он может быть очень вычислительно затратным, особенно при большом количестве гиперпараметров и значений.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой инструмент в библиотеке scikit-learn позволяет автоматически подбирать гиперпараметры с помощью кросс-валидации?",
        "answer": "GridSearchCV или RandomizedSearchCV.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой метод подбора гиперпараметров может адаптировать свою стратегию поиска на основе предыдущих итераций?",
        "answer": "Байесовская оптимизация.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Что такое \"валидационная кривая\"?",
        "answer": "Это график, показывающий, как качество модели (например, точность) изменяется в зависимости от значения одного гиперпараметра, при фиксированных значениях остальных.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Какой метод может быть предпочтительнее, если у вас есть ограниченное количество вычислительных ресурсов?",
        "answer": "Random Search или Байесовская оптимизация.",
        "doc": "podbor-giperparametrov.md"
      },
      {
        "question": "Что такое проксимальные методы?",
        "answer": "Класс оптимизационных алгоритмов, используемых для минимизации негладких, часто составных функций (f(x) = g(x) + h(x)), особенно когда компоненту h(x) можно локализовать и она сравнительно проста. Они основаны на вычислении проксимального оператора.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Что такое прокс-оператор функции h?",
        "answer": "Прокс-оператор функции h определяется как argmin по y выражения (h(y) + (1/2t) * ||x - y||^2). Это точка, которая балансирует между минимизацией h и близостью к точке x.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какова идея проксимального градиентного метода (PGM)?",
        "answer": "PGM используется для оптимизации функций вида f(x) = g(x) + h(x), где g — гладкая функция, а h — негладкая, но с доступным прокс-оператором. На каждой итерации выполняется градиентный шаг по g и проксимальный шаг по h.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Для какой задачи используется алгоритм ISTA?",
        "answer": "ISTA (Iterative Shrinkage-Thresholding Algorithm) используется для решения задачи L1-регрессии (регрессии с L1-регуляризацией).",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какую роль играет soft thresholding в ISTA?",
        "answer": "Soft thresholding — это аналитическая форма прокс-оператора для L1-нормы. В ISTA он применяется поэлементно к результату градиентного шага, что способствует разреженности решения (обнулению некоторых весов).",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какова связь между градиентным спуском и методом проксимальной минимизации?",
        "answer": "Обычный градиентный спуск можно интерпретировать как дискретизацию непрерывного градиентного потока. Метод проксимальной минимизации использует более стабильную обратную схему Эйлера, что приводит к необходимости решать задачу оптимизации на каждом шаге, включающую прокс-оператор.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какие преимущества проксимальных методов?",
        "answer": "Они теоретически обоснованы для выпуклой оптимизации, потенциально более численно стабильны, эффективно решают задачи композитной оптимизации (например, с L1-регуляризацией) и способствуют получению разреженных решений.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какой пример практического применения проксимальных методов упоминается?",
        "answer": "В статье упоминается, что предсказание CTR в Google в 2012 году базировалось на вычислении soft threshold как прокс-оператора, что связано с вариациями алгоритма ISTA.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Какой метод используется для численного решения градиентного потока в проксимальной минимизации?",
        "answer": "Обратная схема Эйлера (backward Euler scheme), которая более численно стабильна по сравнению с обычной схемой Эйлера.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "Почему проксимальные методы полезны для L1-регуляризации?",
        "answer": "Потому что прокс-оператор для L1-нормы (soft thresholding) имеет аналитическое решение, что делает его эффективным элементом в PGM, и способствует разреженности модели.",
        "doc": "proksimalnye-metody.md"
      },
      {
        "question": "В чём основная идея регуляризации в онлайн-обучении, описанной в документе?",
        "answer": "Регуляризация используется не только для стабилизации обучения, но и для наложения ограничений на получаемое решение, например, для достижения разреженности модели.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Что означает идея 'неразложения' регуляризаторов в субградиентную оценку?",
        "answer": "Идея состоит в том, чтобы заменить на субградиентную оценку только гладкую часть функционала, а регуляризатор оставить нетронутым, чтобы задача оптимизации решалась аналитически и потенциально более точно.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какие три основных типа регуляризаторов упоминаются в документе?",
        "answer": "L1-норма (и её собрат L2), проекция на выпуклое множество, и комбинации.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какой алгоритм использует идею 'неразложения' и включает L1-регуляризацию?",
        "answer": "Composite Objective FTRL.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какой метод используется для аналитического решения задачи с L1-регуляризацией в Composite Objective FTRL?",
        "answer": "Soft thresholding (или shrinkage), которое является прокс-оператором для L1-нормы.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Что такое 'decoupled weight decay' и в чём его преимущество?",
        "answer": "Это метод, при котором штраф за величину весов (L2-регуляризация) добавляется к обновлению весов отдельно от градиентного шага, что упрощает настройку гиперпараметров и делает регуляризацию независимой от адаптивности метода.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какой эффект оказывает 'decoupled weight decay' на оптимизацию?",
        "answer": "Он действует как адаптивный L2-регуляризатор, центрированный относительно нуля, и может быть интерпретирован как проекция, улучшающая стабильность и разреженность.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какие виды зависимости коэффициента регуляризации от номера итерации рассматриваются для L1-регуляризации?",
        "answer": "Fixed (фиксированный), Squared incremental (квадратичный прирост), Linear incremental (линейный прирост).",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какой вид зависимости коэффициента L1-регуляризации считается наиболее полезным на практике и почему?",
        "answer": "Фиксированный. Потому что он позволяет частым, но слабым параметрам выйти из нуля, что соответствует продуктивным требованиям: частые параметры остаются, а редкие и слабые отсекаются.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "Какова цель проекции на выпуклое множество как регуляризатора?",
        "answer": "Ограничить пространство допустимых решений до выпуклого множества, например, шара, чтобы накладывать жёсткие ограничения на норму весов.",
        "doc": "regulyarizaciya-v-onlajn-obuchenii.md"
      },
      {
        "question": "В чём основная идея рекомендаций на основе матричных разложений?",
        "answer": "Идея состоит в том, чтобы представить разреженную матрицу пользователь-объект (например, оценок) как произведение двух низкоранговых матриц, представляющих скрытые (латентные) признаки пользователей и объектов. Это позволяет предсказывать рейтинги для отсутствующих оценок и рекомендовать объекты с наивысшим предсказанным рейтингом.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Что такое явный и неявный фидбек в контексте рекомендательных систем?",
        "answer": "Явный фидбек — это прямые оценки пользователей (например, рейтинги, лайки), отражающие степень интереса. Неявный фидбек — это действия, не являющиеся прямой оценкой (например, просмотры, клики), и он более шумный, так как не всегда отражает положительное отношение.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Какова основная идея алгоритма ALS (Alternating Least Squares)?",
        "answer": "ALS оптимизирует функцию потерь, попеременно фиксируя одну из матриц (пользователей или объектов) и решая задачу наименьших квадратов для другой. Это позволяет эффективно находить латентные представления, используя аналитические решения.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "В чём отличие IALS (Implicit ALS) от обычного ALS?",
        "answer": "IALS работает с неявным фидбеком, заменяя пропуски в матрице на нули и вводя веса (confidence), отражающие степень уверенности в предпочтениях пользователя, что позволяет учитывать гораздо больше данных.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Какой подход используется в модели SLIM?",
        "answer": "SLIM использует линейную разреженную модель, где ответ формируется как взвешенная сумма истории взаимодействий пользователя. Модель обучается с L1-регуляризацией, чтобы получить разреженную матрицу весов, отражающую схожесть объектов.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Какие преимущества имеют методы матричной факторизации при генерации кандидатов в рекомендациях?",
        "answer": "Они позволяют эффективно вычислять релевантность множества объектов для пользователя, перемножая их латентные векторы. Это используется для быстрого отбора топ-кандидатов, что особенно полезно в системах с жёсткими временными ограничениями.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Что такое FunkSVD и чем он отличается от ALS?",
        "answer": "FunkSVD — это метод, предложенный Саймоном Функом, который моделирует рейтинги как скалярное произведение латентных векторов. В отличие от ALS, он использует стохастический градиентный спуск для оптимизации, а не аналитические решения задач наименьших квадратов.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Как SVD++ учитывает неявный фидбек?",
        "answer": "SVD++ расширяет модель, добавляя к латентному представлению пользователя слагаемое, отражающее историю его неявных взаимодействий с объектами, что позволяет учитывать больше информации о предпочтениях.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Какой основной недостаток методов матричной факторизации?",
        "answer": "Они учитывают только информацию о взаимодействиях пользователь-объект, игнорируя атрибуты самих пользователей и объектов, что может быть учтено в контентных методах.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Как можно ускорить поиск рекомендаций на основе латентных векторов в реальном времени?",
        "answer": "Можно использовать структуры данных, такие как индексы для поиска ближайших соседей (например, на основе косинусного расстояния или скалярного произведения), чтобы быстро находить наиболее релевантные объекты для вектора пользователя.",
        "doc": "rekomendacii-na-osnove-matrichnyh-razlozhenij.md"
      },
      {
        "question": "Что такое решающее дерево?",
        "answer": "Решающее дерево — это модель машинного обучения, которая предсказывает значение целевой переменной, применяя последовательность простых решающих правил (предикатов). Оно строит кусочно-постоянную аппроксимацию целевой зависимости.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какие преимущества и недостатки у решающих деревьев?",
        "answer": "Преимущества: простота вычисления предсказаний, хорошая интерпретируемость. Недостатки: высокая склонность к переобучению, низкая обобщающая способность, неспособность к экстраполяции.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Почему задача построения оптимального решающего дерева считается сложной?",
        "answer": "Потому что она является NP-полной задачей, и не существует известных алгоритмов, способных решить её за полиномиальное время.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какой алгоритм используется для построения решающего дерева на практике?",
        "answer": "На практике используется жадный алгоритм, который строит дерево поэтапно, на каждом шаге выбирая наилучшее разбиение на основе критерия ветвления.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какие критерии ветвления используются в задаче классификации?",
        "answer": "Часто используемые критерии: энтропия (информационная), критерий Джини, ошибка классификации (misclassification error).",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какой критерий ветвления используется в задаче регрессии с MSE?",
        "answer": "Для MSE информативность (impurity) в листе оценивается как дисперсия таргетов, а критерий ветвления максимизирует уменьшение этой дисперсии.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Как обрабатываются пропущенные значения при обучении решающего дерева?",
        "answer": "Объекты с пропущенными значениями могут отправляться в оба поддерева с весами, пропорциональными количеству объектов в каждом поддереве без пропусков.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какие методы регуляризации применяются к решающим деревьям?",
        "answer": "Методы включают ограничение максимальной глубины, минимального количества объектов в листе, максимального количества листьев, и требование минимального улучшения качества при разбиении.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Как можно ускорить построение решающего дерева?",
        "answer": "Можно использовать динамическое программирование (например, сортировка + проход для подсчёта статистик) и гистограммный метод (дискретизация признаков).",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Какой способ упорядочивания значений категориального признака используется для бинарной классификации?",
        "answer": "Значения можно упорядочить по неубыванию доли объектов положительного класса (класса 1) среди объектов с данным значением признака.",
        "doc": "reshayushchiye-derevya.md"
      },
      {
        "question": "Что такое нейрокасательное ядро (NTK)?",
        "answer": "NTK (Neural Tangent Kernel) — это функция, описывающая динамику обучения бесконечно широкой нейронной сети. Она определяется как скалярное произведение градиентов модели по всем параметрам в фиксированной точке и определяет эволюцию предсказаний сети в процессе градиентного спуска.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "В чём разница между NTK-параметризацией и стандартной параметризацией нейронной сети?",
        "answer": "NTK-параметризация масштабирует веса так, что при стремлении ширины к бесконечности эмпирическое NTK сходится к фиксированному пределу. В стандартной параметризации NTK расходится с шириной, и ядро эволюционирует во времени.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Как NTK связано с обучением нейронной сети?",
        "answer": "NTK описывает, как изменяются предсказания сети в процессе градиентного спуска. В пределе бесконечной ширины при NTK-параметризации динамика предсказаний становится линейной и полностью определяется NTK, что позволяет аналитически проанализировать обучение.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Какие преимущества даёт анализ нейронных сетей через NTK?",
        "answer": "NTK позволяет доказать сходимость к глобальному минимуму на обучающей выборке для переобученных сетей и предоставляет инструмент для анализа обобщающей способности. Также NTK может использоваться как ядро в ядровых методах.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Что такое эмпирическое NTK и как оно связано с предельным NTK?",
        "answer": "Эмпирическое NTK — это NTK, вычисленное для конкретной конечной сети. При стремлении ширины к бесконечности (при правильной параметризации) эмпирическое NTK сходится к предельному NTK, которое не зависит от времени и инициализации.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Как NTK может помочь в диагностике проблем с обучением?",
        "answer": "Плохая обусловленность (например, малые или очень большие собственные значения) NTK в инициализации может указывать на проблемы с обучением, такие как медленная сходимость или нестабильность.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Можно ли использовать NTK в ядровых методах, таких как SVM?",
        "answer": "Да, предельное NTK можно использовать как ядро в ядровых методах, таких как SVM или ядровая регрессия, что позволяет анализировать поведение нейронных сетей с помощью более простых и устойчивых моделей.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Как NTK связано с ядровыми методами?",
        "answer": "NTK можно рассматривать как специфическое ядро, которое возникает в пределе бесконечной ширины. Это позволяет применять теорию ядровых методов к анализу нейронных сетей.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Что утверждает Master theorem в контексте тензорных программ и NTK?",
        "answer": "Master theorem утверждает, что при стремлении ширины тензорной программы к бесконечности, выражения, включая NTK, сходятся к детерминированному пределу, который можно вычислить рекуррентно.",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Как ведёт себя NTK в задачах классификации при стандартной параметризации?",
        "answer": "При стандартной параметризации NTK расходится с шириной, но для задач классификации это может быть не критично, так как важны не абсолютные значения предсказаний, а их знак или соотношение (например, индекс максимального логита).",
        "doc": "seti-beskonechnoj-shiriny.md"
      },
      {
        "question": "Что такое стохастический градиентный спуск (SGD)?",
        "answer": "SGD — это метод оптимизации, используемый для минимизации функции, которая может быть представлена как сумма функций (например, эмпирический риск). На каждой итерации он использует не градиент всей функции, а стохастический градиент, который является несмещённой оценкой градиента, вычисленной по одному или нескольким случайно выбранным слагаемым.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какова основная идея методов редукции дисперсии, таких как SVRG и SAGA?",
        "answer": "Методы редукции дисперсии, такие как SVRG и SAGA, модифицируют правило обновления стохастического градиента, чтобы уменьшить его дисперсию по мере приближения к решению. Это позволяет алгоритму сходиться линейно к точному решению, в отличие от стандартного SGD, который сходится к окрестности решения.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Почему SGD не сходится линейно к точному решению?",
        "answer": "SGD не сходится линейно к точному решению из-за постоянной дисперсии стохастического градиента, даже когда текущая точка находится вблизи решения. Эта дисперсия вызывает осцилляции, не позволяющие методу стабильно сойтись к точке минимума.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какова разница между SVRG и L-SVRG?",
        "answer": "SVRG обновляет контрольную точку (для вычисления полного градиента) через фиксированное количество итераций m. L-SVRG (Loopless SVRG) обновляет эту точку случайным образом с небольшой вероятностью p на каждой итерации, что делает длину цикла случайной.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какой основной недостаток SAGA?",
        "answer": "Основной недостаток SAGA — необходимость хранения градиентов для каждого слагаемого (n векторов), что может быть затратно по памяти для больших датасетов.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какой тип задач подходит для анализа SGD с предположением о выпуклых гладких стохастических реализациях?",
        "answer": "Это предположение подходит для задач, где целевая функция может быть представлена как математическое ожидание функций, каждая из которых выпукла и гладка (например, задачи линейной или логистической регрессии с усреднением по датасету).",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Что утверждает теорема о сходимости SGD с постоянным шагом для сильно выпуклых функций?",
        "answer": "Теорема утверждает, что SGD с постоянным шагом сходится линейно к окрестности решения, размер которой пропорционален дисперсии стохастического градиента, делённой на произведение параметра сильной выпуклости и квадрата размера шага.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какова цель стохастического градиента в SGD?",
        "answer": "Цель стохастического градиента — предоставить несмещённую оценку истинного градиента целевой функции, но с меньшими вычислительными затратами на одну итерацию по сравнению с вычислением полного градиента.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Почему методы редукции дисперсии могут быть предпочтительнее обычного градиентного спуска (GD) в некоторых задачах?",
        "answer": "В задачах с большим количеством данных методы редукции дисперсии могут быть предпочтительнее, потому что они имеют более низкую вычислительную стоимость итерации по сравнению с GD (который требует вычисления полного градиента) и при этом обеспечивают линейную сходимость к точному решению.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Какой метод, SVRG или SAGA, требует вычисления полного градиента в контрольной точке?",
        "answer": "SVRG требует вычисления полного градиента в контрольной точке на каждом внутреннем цикле. SAGA этого не требует, но хранит градиенты всех слагаемых.",
        "doc": "shodimost-sgd.md"
      },
      {
        "question": "Что такое свёрточная нейронная сеть (CNN)?",
        "answer": "Свёрточная нейронная сеть — это тип нейронной сети, использующий свёрточные слои для обработки данных, обычно изображений. Свёртки позволяют сети эффективно учитывать пространственную структуру данных и быть устойчивой к сдвигам.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Какие проблемы решают свёртки по сравнению с полносвязными сетями?",
        "answer": "Свёртки решают проблему большого количества параметров и игнорирования пространственной структуры данных (например, инвариантности к сдвигам), характерных для полносвязных сетей.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Что такое ядро свёртки?",
        "answer": "Ядро свёртки — это обучаемый тензор (матрица/фильтр), который «скользит» по входному изображению, выполняя поэлементное умножение и суммирование, чтобы получить карту признаков.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Как работает Max Pooling?",
        "answer": "Max Pooling — это операция, которая уменьшает размер карты признаков, выбирая максимальное значение в каждом окне фиксированного размера (например, 2x2), двигаясь с определённым шагом.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Что такое receptive field в контексте CNN?",
        "answer": "Receptive field — это область входного изображения, на которую «смотрит» конкретный нейрон в свёрточной сети. Он определяет, какая часть изображения влияет на активацию этого нейрона.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Как увеличить receptive field без увеличения размера ядра?",
        "answer": "Receptive field можно увеличить, используя композицию свёрточных слоёв или dilated convolution (свёртка с дырками), где ядро применяется с пропусками.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Что такое residual connection и зачем он нужен?",
        "answer": "Residual connection — это прямое соединение, которое пропускает один или несколько слоёв и суммируется с выходом этих слоёв. Он помогает решить проблему затухания градиентов в глубоких сетях, облегчая обучение.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Какие популярные методы регуляризации используются в CNN?",
        "answer": "Популярные методы включают аугментации данных (например, сдвиги, повороты), label smoothing, mixup, dropout и классические L1/L2 регуляризации.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Что такое Global Average Pooling (GAP)?",
        "answer": "GAP — это операция, при которой усредняются значения по пространственным измерениям карт признаков, превращая их в вектор фиксированного размера, что полезно для уменьшения количества параметров и работы с изображениями разных размеров.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Какие задачи, помимо классификации, можно решать с помощью CNN?",
        "answer": "CNN можно использовать для сегментации, детекции объектов, понимания видео, metric learning и других задач, связанных с пространственно-организованными данными.",
        "doc": "svyortochnye-nejroseti.md"
      },
      {
        "question": "Что такое истинный риск и эмпирический риск в контексте машинного обучения?",
        "answer": "Истинный риск — это математическое ожидание функции потерь на всех возможных данных, взятых из истинного распределения. Эмпирический риск — это среднее значение функции потерь на конкретной обучающей выборке. Цель обучения — минимизировать истинный риск, но поскольку истинное распределение неизвестно, минимизируют эмпирический риск.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Почему равномерные оценки (uniform bounds) могут быть бесполезны для нейронных сетей?",
        "answer": "Равномерные оценки могут быть бесполезны (vacuous), если класс моделей настолько сложен, что в нём существуют модели, идеально запоминающие обучающую выборку (нулевой эмпирический риск), но имеющие высокий истинный риск. Это характерно для глубоких нейронных сетей, которые обладают высокой емкостью.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Что такое VC-размерность и как она связана с обобщающей способностью?",
        "answer": "VC-размерность — это классическая мера сложности класса моделей. Она используется в равномерных оценках разницы между истинным и эмпирическим рисками. Чем выше VC-размерность, тем сложнее класс моделей и, потенциально, хуже его обобщающая способность.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Что такое implicit bias в контексте обучения нейронных сетей?",
        "answer": "Implicit bias — это явление, при котором алгоритм обучения (например, градиентный спуск) среди всех моделей с нулевым эмпирическим риском предпочитает определённые. Это объясняет, почему градиентный спуск находит модели с хорошей обобщающей способностью, а не просто запоминающие данные.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Почему градиентный спуск может сходиться к глобальному минимуму в нейронных сетях?",
        "answer": "Хотя в общем случае функция потерь в нейронных сетях не является выпуклой, для переобученных сетей (где много параметров) часто удается доказать, что все локальные минимумы являются глобальными. Также градиентный спуск с высокой вероятностью не сходится к седловым точкам.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Какова цель PAC-байесовских оценок?",
        "answer": "PAC-байесовские оценки позволяют учитывать предпочтения алгоритма обучения (implicit bias), рассматривая распределения на моделях (априорное и апостериорное) и используя KL-дивергенцию для оценки разницы между истинным и эмпирическим рисками.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Что показывает зависимость истинного и эмпирического риска от ширины сети?",
        "answer": "Эмпирический риск обычно убывает с увеличением ширины до нуля. Раньше считалось, что истинный риск будет после этого расти (переобучение), но на практике он продолжает убывать и выходит на асимптоту, что противоречит классическим равномерным оценкам.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Какие проблемы возникают при оптимизации нейронных сетей?",
        "answer": "Проблемы включают в себя возможное наличие седловых точек, необходимость инициализации весов, чтобы избежать затухания или взрыва градиентов, и отсутствие гарантий сходимости к глобальному минимуму для произвольных сетей.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Что такое эксперимент с перемешиванием меток и что он демонстрирует?",
        "answer": "Это эксперимент, в котором часть обучающей выборки случайным образом помечается, чтобы показать, что модели (например, VGG) могут легко запомнить обучающие данные даже с неправильными метками, что указывает на высокую емкость модели и ограниченность равномерных оценок.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Какие инициализации весов упоминаются как способ борьбы с проблемами градиентов?",
        "answer": "Упоминаются инициализации Глоро (Xavier Glorot) и Хе (Kaiming He), которые помогают избежать затухания или взрыва градиентов при обучении глубоких сетей.",
        "doc": "teoriya-glubokogo-obucheniya-vvedenie.md"
      },
      {
        "question": "Почему инициализация весов нейронной сети нулями приводит к проблемам?",
        "answer": "Инициализация весов нулями приводит к симметрии между нейронами одного слоя: они будут вычислять одинаковые градиенты и обновляться одинаково. Это означает, что все нейроны в слое будут вести себя одинаково, и обучение эффективно не произойдёт, так как не будет разнообразия в представлениях.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Что такое Xavier инициализация и для каких функций активации она подходит?",
        "answer": "Xavier инициализация — это метод инициализации весов, при котором дисперсия весов выбирается так, чтобы сохранять дисперсию активаций и градиентов при прохождении сигнала вперёд и назад. Она хорошо подходит для симметричных функций активации, таких как tanh.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Что такое Kaiming (He) инициализация и когда её стоит использовать?",
        "answer": "Kaiming инициализация — это метод инициализации весов, адаптированный под функцию активации ReLU. Она учитывает, что ReLU обнуляет половину входов, и корректирует дисперсию инициализации, чтобы компенсировать это. Используется для сетей с ReLU и её вариациями.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Как работает Dropout и зачем он нужен?",
        "answer": "Dropout — это метод регуляризации, при котором на каждом шаге обучения случайным образом обнуляются («выключаются») некоторые нейроны. Это предотвращает чрезмерную зависимость от отдельных нейронов, заставляя сеть быть более устойчивой и обобщать лучше, уменьшая переобучение.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Что такое Batch Normalization и какую проблему он решает?",
        "answer": "Batch Normalization — это метод, который нормализует активации каждого слоя на батче, приводя их к нулевому среднему и единичной дисперсии. Это стабилизирует процесс обучения, позволяя использовать большие шаги обучения и ускоряя сходимость, а также уменьшая переобучение.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Какие типы регуляризации существуют для нейронных сетей?",
        "answer": "Типы регуляризации: 1) через функцию потерь (например, L2-регуляризация, энтропийные штрафы), 2) через структуру сети (например, Dropout, Batch Normalization), 3) через изменение данных (например, аугментация данных).",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Почему при инициализации случайными числами важно контролировать дисперсию?",
        "answer": "Контроль дисперсии при инициализации важен, чтобы избежать исчезновения или взрыва градиентов в глубоких сетях. Если дисперсия слишком высока, активации могут насыщать функции активации, а если слишком низка — сигнал будет затухать.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Как Weight Decay влияет на обучение нейронной сети?",
        "answer": "Weight Decay добавляет штраф за большие значения весов к функции потерь, что помогает уменьшить переобучение, поощряя модель использовать более простые и обобщающие решения.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Что такое аугментация данных и зачем она нужна?",
        "answer": "Аугментация данных — это процесс искусственно увеличивать обучающую выборку за счёт преобразований (например, повороты, шум). Это помогает модели быть устойчивой к вариациям в данных и улучшает обобщающую способность.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Какие преимущества даёт использование Batch Normalization?",
        "answer": "Batch Normalization ускоряет обучение, позволяет использовать большие шаги обучения, улучшает сходимость, стабилизирует градиенты и уменьшает переобучение.",
        "doc": "tonkosti-obucheniya.md"
      },
      {
        "question": "Что такое механизм внимания (attention) в трансформерах?",
        "answer": "Механизм внимания (attention) — это способ, при котором каждый элемент последовательности взаимодействует с каждым, позволяя модели определять, какие части входных данных наиболее важны для текущего элемента. Это достигается через вычисление запросов (queries), ключей (keys) и значений (values).",
        "doc": "transformery.md"
      },
      {
        "question": "Какова основная идея self-attention?",
        "answer": "Self-attention позволяет каждому элементу последовательности напрямую взаимодействовать со всеми другими элементами, вычисляя веса важности (аттеншн-веса) на основе скалярных произведений запросов и ключей, а затем взвешенно суммируя значения.",
        "doc": "transformery.md"
      },
      {
        "question": "Какие основные компоненты используются в вычислении self-attention?",
        "answer": "Основные компоненты — это запросы (Q), ключи (K) и значения (V), которые получаются путём умножения входных эмбеддингов на обучаемые матрицы весов W_Q, W_K, W_V.",
        "doc": "transformery.md"
      },
      {
        "question": "Что такое multi-head attention?",
        "answer": "Multi-head attention — это механизм, при котором применяется несколько параллельных слоёв внимания (голов), каждая из которых может извлекать разные типы зависимостей, а затем результаты объединяются.",
        "doc": "transformery.md"
      },
      {
        "question": "Как устроена архитектура трансформера?",
        "answer": "Архитектура состоит из энкодера и декодера. Энкодер и декодер состоят из стека блоков, в каждом из которых есть слои self-attention, feed-forward сети и residual connections с нормализацией.",
        "doc": "transformery.md"
      },
      {
        "question": "Какова роль позиционных эмбеддингов?",
        "answer": "Позиционные эмбеддинги добавляются к входным эмбеддингам, чтобы модель могла учитывать порядок элементов в последовательности, так как сам механизм внимания инвариантен к порядку.",
        "doc": "transformery.md"
      },
      {
        "question": "В чём разница между BERT и GPT?",
        "answer": "BERT использует двунаправленное внимание (энкодер), что позволяет ему учитывать контекст слева и справа, и обучается на masked language modeling. GPT использует однонаправленное (авторегрессивное) внимание (декодер) и обучается на предсказание следующего токена.",
        "doc": "transformery.md"
      },
      {
        "question": "Какие проблемы решает архитектура трансформера по сравнению с RNN?",
        "answer": "Трансформеры решают проблемы долгосрочных зависимостей и трудностей с параллелизацией, характерные для RNN. Они позволяют обрабатывать всю последовательность параллельно и эффективно моделировать зависимости между отдалёнными элементами.",
        "doc": "transformery.md"
      },
      {
        "question": "Какова вычислительная сложность self-attention?",
        "answer": "Вычислительная сложность self-attention составляет O(n^2) по длине последовательности n, что является квадратичной зависимостью.",
        "doc": "transformery.md"
      },
      {
        "question": "Где ещё, помимо обработки текста, применяются трансформеры?",
        "answer": "Трансформеры применяются в компьютерном зрении (например, Vision Transformer), генерации изображений (DALL-E), обучении с подкреплением (Decision Transformer) и других областях.",
        "doc": "transformery.md"
      },
      {
        "question": "Что такое вариационный автоэнкодер (VAE)?",
        "answer": "VAE (Variational Autoencoder) — это генеративная модель, которая учится приближать распределение данных через обучение энкодера (который кодирует данные в латентное пространство) и декодера (который восстанавливает данные из латентного представления). Обучение основано на максимизации ELBO (Evidence Lower BOund), что позволяет модели генерировать новые данные, семплируя из латентного пространства.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Какова основная идея VAE?",
        "answer": "Основная идея VAE — обучить вероятностную модель, которая может генерировать новые данные. Это достигается за счёт введения латентной переменной z, распределённой по априорному распределению p(z), и обучении условных распределений p(x|z) (декодер) и q(z|x) (аппарат для инференса, энкодер), максимизируя ELBO.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Что такое ELBO в контексте VAE?",
        "answer": "ELBO (Evidence Lower BOund) — это нижняя оценка логарифма правдоподобия, которую VAE максимизирует при обучении. Она состоит из двух компонент: реконструкционной потери (насколько хорошо модель восстанавливает данные) и KL-дивергенции между апостериорным и априорным распределением в латентном пространстве.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Какие компоненты входят в архитектуру VAE?",
        "answer": "Архитектура VAE включает энкодер (нейронную сеть, преобразующую вход x в параметры распределения в латентном пространстве z) и декодер (нейронную сеть, преобразующую z обратно в x). Также важна априорная гипотеза о распределении z, обычно берётся стандартное нормальное распределение.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Что такое VQ-VAE?",
        "answer": "VQ-VAE (Vector Quantised VAE) — это модификация VAE, в которой латентное пространство дискретно. Вместо непрерывного z используется квантование: энкодер выдаёт непрерывные векторы, которые затем заменяются ближайшими векторами из обучаемого кодового словаря.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Какие преимущества даёт использование дискретного латентного пространства в VQ-VAE?",
        "answer": "Дискретное латентное пространство позволяет использовать мощные модели, разработанные для дискретных данных (например, Transformer), для моделирования априорного распределения. Это может улучшить качество генерации, особенно для изображений.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Что такое DALL-E и как он связан с VAE?",
        "answer": "DALL-E — это модель, генерирующая изображения по текстовому описанию. Она использует идеи VAE: сначала обучается дискретизованный VAE (dVAE) для сжатия изображений в дискретные кодовые векторы, а затем Transformer моделирует совместное распределение текста и этих кодов.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Как работает Conditional VAE (CVAE)?",
        "answer": "CVAE — это VAE, обученное с условием на дополнительную информацию (например, метку класса). Это позволяет генерировать данные определённого класса, подавая метку на вход как энкодера, так и декодера.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Что такое репараметризационный трюк (reparameterization trick)?",
        "answer": "Репараметризационный трюк — это метод, позволяющий дифференцировать семплирование из распределения. Вместо прямого семплирования z из q(z|x), генерируется случайный шум epsilon, и z вычисляется как детерминированная функция от epsilon и параметров распределения.",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Какие модификации VAE упоминаются в документе?",
        "answer": "Упоминаются VQ-VAE, VQ-VAE-2 и DALL-E, а также Conditional VAE (CVAE).",
        "doc": "variational-autoencoder-(vae).md"
      },
      {
        "question": "Что такое вероятностное пространство?",
        "answer": "Вероятностное пространство — это математическая конструкция, задаваемая тремя компонентами: пространством элементарных исходов (Ω), алгеброй (или сигма-алгеброй) событий (F) и вероятностной мерой (P), определённой на этой алгебре. Оно формализует понятие вероятности.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое дискретное и непрерывное распределение?",
        "answer": "Дискретное распределение — это распределение вероятностей, сосредоточенное на не более чем счётном множестве исходов. Непрерывное распределение — это распределение, которое можно описать плотностью вероятности на числовой прямой.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Как определяется математическое ожидание и дисперсия случайной величины?",
        "answer": "Математическое ожидание — это средневзвешенное значение случайной величины. Для дискретной величины: E[X] = Σ x * P(X = x). Для непрерывной: E[X] = ∫ x * f(x) dx. Дисперсия — это E[(X - E[X])^2], мера разброса значений случайной величины относительно её среднего.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое функция распределения (CDF)?",
        "answer": "Функция распределения F(x) случайной величины X — это функция, задающая вероятность P(X <= x). Она неубывающая, стремится к 0 при x -> -∞ и к 1 при x -> +∞.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое плотность вероятности (PDF)?",
        "answer": "Плотность вероятности — это функция f(x), такая что вероятность попадания случайной величины в интервал [a, b] равна интегралу от f(x) по этому интервалу. Производная функции распределения, если существует.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Какие свойства имеет нормальное распределение?",
        "answer": "Нормальное (гауссовское) распределение задаётся плотностью, зависящей от параметров μ (среднее) и σ^2 (дисперсия). Плотность симметрична и имеет колоколообразную форму. Является унимодальным.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое биномиальное распределение?",
        "answer": "Биномиальное распределение описывает количество успехов в n независимых испытаниях Бернулли, каждое из которых имеет вероятность успеха p. Параметры: n (число испытаний) и p (вероятность успеха).",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое распределение Бернулли?",
        "answer": "Распределение Бернулли — это простейшее дискретное распределение с двумя исходами: 1 (успех) с вероятностью p и 0 (неудача) с вероятностью 1-p.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое ковариация и коэффициент корреляции?",
        "answer": "Ковариация измеряет линейную зависимость между двумя случайными величинами. Коэффициент корреляции — это нормированная ковариация, принимающая значения от -1 до 1, где 0 означает отсутствие линейной корреляции.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Что такое медиана и мода распределения?",
        "answer": "Медиана — это значение, которое делит распределение на две равные вероятностные половины. Мода — это значение, в котором функция плотности (или вероятности) достигает максимума.",
        "doc": "veroyatnostnye-raspredeleniya.md"
      },
      {
        "question": "Как вероятностный подход связан с оптимизацией функции потерь?",
        "answer": "Вероятностный подход к машинному обучению интерпретирует обучение как задачу оценки параметров модели с помощью метода максимального правдоподобия. Максимизация функции правдоподобия эквивалентна минимизации отрицательного логарифма правдоподобия, который часто совпадает с функцией потерь (например, MSE для нормального шума, MAE для лапласовского шума).",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Какая вероятностная модель соответствует MSE как функции потерь?",
        "answer": "Модель линейной регрессии с аддитивным гауссовским (нормальным) шумом. В этом случае минимизация MSE эквивалентна максимизации правдоподобия при условии, что ошибки распределены нормально.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Какая вероятностная модель соответствует MAE как функции потерь?",
        "answer": "Модель линейной регрессии с аддитивным шумом, распределённым по Лапласу. В этом случае минимизация MAE эквивалентна максимизации правдоподобия.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Что такое условное распределение в вероятностных моделях?",
        "answer": "Условное распределение — это вероятностное распределение целевой переменной (таргета), заданное значениями признаков. Оно описывает, как распределён таргет при известных значениях признаков, например, P(y|x, θ).",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Как логистическая регрессия соотносится с вероятностной моделью?",
        "answer": "Логистическая регрессия моделирует вероятность принадлежности к положительному классу с помощью сигмоиды от линейной комбинации признаков. Она предполагает, что таргет подчиняется распределению Бернулли с параметром, определяемым сигмоидой.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Что такое softmax и для чего он используется?",
        "answer": "Softmax — это функция, обобщающая сигмоиду на многоклассовый случай. Она преобразует вектор логитов в вектор вероятностей, сумма которых равна 1. Используется для предсказания вероятностей классов в задачах многоклассовой классификации.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Почему оценки вероятностей в классических моделях могут быть неадекватными?",
        "answer": "Потому что параметры модели настраиваются на максимизацию правдоподобия или минимизацию функции потерь, зависящей от меток классов, а не на точное восстановление истинных вероятностей. Это может привести к смещенным оценкам, особенно в задачах, чувствительных к вероятностям, а не только к предсказанию меток.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Что такое графическая модель?",
        "answer": "Графическая модель — это вероятностная модель, в которой зависимости между случайными величинами представлены в виде графа. Узлы — это случайные величины, а рёбра — статистические зависимости между ними.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Какой вид принимает вероятностная модель с аддитивным шумом?",
        "answer": "y = f(x, θ) + ε, где f — функция, описывающая зависимость, θ — параметры модели, а ε — случайный шум с известным распределением.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Почему в вероятностных моделях предпочтительно использовать условное матожидание как точечное предсказание?",
        "answer": "Потому что условное матожидание E[y|x] минимизирует среднеквадратичную ошибку (MSE) среди всех возможных точечных предсказаний. Для симметричных распределений (например, нормального) оно совпадает с модой и медианой.",
        "doc": "veroyatnostnyj-podhod-v-ml.md"
      },
      {
        "question": "Что такое временной ряд?",
        "answer": "Временной ряд — это последовательность значений признаков, изменяющихся во времени, полученных в определённые моменты времени.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какова задача прогнозирования временного ряда?",
        "answer": "Задача прогнозирования временного ряда заключается в построении функции, которая на основе известных значений ряда до определённого момента времени предсказывает его будущие значения.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие компоненты выделяют при декомпозиции временного ряда?",
        "answer": "При декомпозиции временного ряда выделяют три компоненты: тренд, сезонность и ошибку (остаток).",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Что такое STL-декомпозиция?",
        "answer": "STL-декомпозиция — это метод декомпозиции временного ряда на тренд, сезонность и остаток, использующий LOESS-регрессию. Название происходит от Seasonal-Trend decomposition using LOESS.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие стратегии прогнозирования используются при построении прогноза на несколько шагов вперёд?",
        "answer": "Используются три основные стратегии: рекурсивная (прогноз на один шаг, используемый как признак для следующего), прямая (отдельная модель для каждого шага горизонта) и гибридная (сочетание первых двух).",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие признаки можно использовать для прогнозирования временного ряда?",
        "answer": "Можно использовать: даты (временные признаки), предыдущие значения ряда, скользящие статистики (среднее, медиана, и т.д.), сезонные признаки, счётчики по категориальным признакам.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие метрики используются для оценки качества прогноза временного ряда?",
        "answer": "Часто используются MSE (средняя квадратичная ошибка), MAE (средняя абсолютная ошибка), MAPE (средняя абсолютная ошибка в процентах), WMAPE (взвешенная средняя абсолютная ошибка в процентах).",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Почему нельзя использовать стандартную кросс-валидацию для временных рядов?",
        "answer": "Стандартная кросс-валидация перемешивает данные, нарушая временной порядок. Для временных рядов используются специальные схемы, где обучающая и тестовая выборки разделяются по времени.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие схемы кросс-валидации применяются для временных рядов?",
        "answer": "Применяются схемы, при которых обучение происходит на начальных временных отрезках, а тест — на следующих за ними. Размер обучающего окна может увеличиваться (развёрнутая схема) или оставаться фиксированным (скользящее окно).",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "Какие преимущества и недостатки у ML-моделей для прогнозирования временных рядов?",
        "answer": "Преимущества: гибкость, возможность использовать экзогенные признаки, возможность прогнозировать несколько рядов. Недостатки: сложность построения предсказательных интервалов, потенциально худшее качество по сравнению с классическими моделями, сложности с интерпретацией.",
        "doc": "vremennye-ryady.md"
      },
      {
        "question": "В чём разница между дискриминативным и генеративным моделированием?",
        "answer": "Дискриминативное моделирование предсказывает характеристики объектов (например, метки класса) по признакам, оценивая условную вероятность P(y|x). Генеративное моделирование учится приближать распределение данных и может генерировать новые объекты, оценивая P(x) или P(x|y).",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Что такое латентное пространство в генеративных моделях?",
        "answer": "Латентное пространство — это пространство, в котором располагаются скрытые (латентные) представления объектов. В генеративных моделях генератор часто принимает на вход вектор случайных значений из этого пространства, чтобы создать новый объект.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Какие задачи можно решать с помощью генеративных моделей?",
        "answer": "Генеративные модели применяются для аугментации данных, редактирования изображений (например, изменение разрешения, inpainting), генерации изображений по текстовому описанию, создания новых объектов, интерполяции в латентном пространстве и других задач.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Какие бывают подходы к моделированию плотности в генеративных моделях?",
        "answer": "Существуют два основных подхода: 1) Явное моделирование — построение и оценка функции плотности (например, PixelCNN++, VAE, диффузионные модели). 2) Неявное моделирование — семплирование из распределения без оценки плотности (например, GAN).",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Что такое интерполяция в латентном пространстве?",
        "answer": "Интерполяция в латентном пространстве — это процесс получения промежуточных объектов, путём движения по прямой между двумя точками в латентном пространстве и генерации объектов для каждой точки на этой траектории.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Какие примеры генеративных моделей упоминаются в документе?",
        "answer": "Примеры включают VAE (вариационные автокодировщики), GAN (генеративные состязательные сети), диффузионные модели, PixelCNN++, Stable Diffusion, DALL-E 2, а также модели на основе нормализующих потоков.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Почему генеративное моделирование считается более сложным, чем дискриминативное?",
        "answer": "Генеративное моделирование сложнее, потому что модели должны научиться приближать сложное распределение данных и генерировать реалистичные объекты, в то время как дискриминативные модели решают более простую задачу предсказания характеристик по признакам.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Какие приложения генеративных моделей существуют в области изображений?",
        "answer": "Приложения включают генерацию изображений по текстовому описанию, повышение разрешения (super-resolution), закрашивание (inpainting), перенос стиля и редактирование изображений.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Что такое аугментация данных с помощью генеративных моделей?",
        "answer": "Аугментация данных с помощью генеративных моделей — это процесс создания новых, синтетических данных, похожих на обучающую выборку, чтобы улучшить обобщающую способность модели и избежать переобучения.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Какие примеры текст-в-изображение моделей упоминаются?",
        "answer": "Примеры включают Stable Diffusion, DALL-E 2, Midjourney и Imagen.",
        "doc": "vvedenie-v-generativnoe-modelirovanie.md"
      },
      {
        "question": "Что такое языковая модель?",
        "answer": "Языковая модель — это вероятностный алгоритм, способный продолжать тексты, моделируя вероятности следующих токенов на основе предыдущего контекста.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Какие существуют подходы к токенизации текста?",
        "answer": "Существуют различные подходы: разбиение по пробелам и знакам препинания (CountVectorizer), BPE (Byte Pair Encoding), и SentencePiece. BPE и SentencePiece позволяют работать с неограниченным словарем, разбивая редкие слова на субтокены.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Как устроены рекуррентные нейронные сети (RNN) и в чём их преимущество перед статистическими моделями?",
        "answer": "RNN обрабатывают последовательности по одному элементу, сохраняя внутреннее состояние. По сравнению со статистическими моделями, они способны учитывать более длинный контекст и генерировать более связные тексты.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "В чём основное преимущество архитектуры трансформеров?",
        "answer": "Трансформеры используют механизм внимания (attention), что позволяет им учитывать связи между всеми токенами в последовательности, не страдая от проблем с длинными зависимостями, характерных для RNN.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Что такое GPT и как он устроен?",
        "answer": "GPT (Generative Pre-trained Transformer) — это языковая модель, использующая декодер-часть архитектуры трансформера. Она обучается предсказывать следующий токен в последовательности и может генерировать текст автoreгрессивно.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Что такое few-shot обучение?",
        "answer": "Few-shot обучение — это подход, при котором модель решает задачу, получая в качестве подсказки лишь несколько примеров в инпуте, без дополнительного обучения.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Что такое Chain-of-Thought (CoT)?",
        "answer": "Chain-of-Thought — это техника, при которой в промт включаются пошаговые рассуждения для решения задачи, что помогает модели лучше справляться с логическими и математическими задачами.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Как обучают инструкционные модели, такие как InstructGPT?",
        "answer": "Обучение включает: 1) предобучение на большом корпусе текста; 2) SFT (supervised fine-tuning) на датасете инструкций; 3) обучение reward-модели на парах ответов; 4) дообучение с помощью методов RL (например, PPO) для выравнивания с предпочтениями человека.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Что такое reward-модель и зачем она нужна?",
        "answer": "Reward-модель — это модель, которая оценивает качество сгенерированного текста. Она обучается предсказывать, какой из двух ответов лучше, и используется для дообучения языковой модели с помощью методов обучения с подкреплением.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Какие архитектурные особенности у LLaMA?",
        "answer": "LLaMA использует pre-нормализацию (RMSNorm), SwiGLU-активацию, роторные (rotary) эмбеддинги для учёта позиции токенов и другие оптимизации для повышения эффективности и качества.",
        "doc": "yazykovye-modeli.md"
      },
      {
        "question": "Что такое задача ранжирования?",
        "answer": "Задача ранжирования — это задача сортировки документов (или других объектов) внутри набора по убыванию релевантности запросу (или пользовательскому интересу).",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Какие бывают типы методов обучения для задачи ранжирования?",
        "answer": "Методы обучения для задачи ранжирования делятся на три типа: поточечный (pointwise), попарный (pairwise) и списочный (listwise).",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "В чём разница между pointwise, pairwise и listwise подходами?",
        "answer": "Pointwise обучает модель предсказывать релевантность отдельного документа. Pairwise обучает модель различать, какой из двух документов релевантнее. Listwise обучает модель учитывать порядок всего списка документов при оптимизации метрики.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Что такое NDCG и как он вычисляется?",
        "answer": "NDCG (Normalized Discounted Cumulative Gain) — это метрика качества ранжирования, которая учитывает порядок релевантных документов и нормализована на идеальное ранжирование. Она вычисляется как DCG при фактическом ранжировании, делённый на DCG при идеальном ранжировании.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Что такое MAP (Mean Average Precision)?",
        "answer": "MAP — это метрика качества ранжирования, равная усреднённому значению Average Precision (AP) по всем запросам. AP — это среднее значение precision на позициях, где стоят релевантные документы.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Что такое MRR (Mean Reciprocal Rank)?",
        "answer": "MRR — это метрика качества ранжирования, равная среднему значению обратных рангов первого релевантного документа по всем запросам.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Как работает метод RankNet?",
        "answer": "RankNet — это попарный метод, который обучает модель предсказывать вероятность того, что один документ релевантнее другого. Он использует сигмоиду от разности скоров для вычисления этой вероятности и минимизирует кросс-энтропию.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Что такое LambdaRank и в чём его цель?",
        "answer": "LambdaRank — это попарный метод, который модифицирует подход RankNet с целью прямой оптимизации недифференцируемых метрик ранжирования, таких как NDCG. Он использует градиенты, основанные на изменении целевой метрики при перестановке пары документов.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Какие популярные признаки используются в задаче ранжирования?",
        "answer": "Популярные признаки: TF-IDF, BM25, косинусная близость между эмбеддингами запроса и документа (например, из DSSM или BERT), метафичи (предсказания других моделей), запросные, документные и запросно-документные признаки.",
        "doc": "zadacha-ranzhirovaniya.md"
      },
      {
        "question": "Что такое многостадийное ранжирование?",
        "answer": "Многостадийное ранжирование — это подход, при котором сначала грубым и быстрым методом (например, TF-IDF) отбираются кандидаты, а затем более сложные и точные модели применяются к отобранному подмножеству документов.",
        "doc": "zadacha-ranzhirovaniya.md"
      }
    ]